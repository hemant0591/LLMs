{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### 1. Log-Probabilities and Training\n\nLLMs output a probability distribution over the vocabulary at every step.\nThe training objective is negative log-likelihood (NLL):\n\n$$\n\\mathcal{L} = - \\sum_t \\log P(x_t \\mid x_{<t})\n$$\n\nThe log-probabilities from the model are later used in decoding algorithms such as beam search and best-of-N sampling.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"### 2. Decoding Algorithms\n\n#### Greedy Search\n\t•\tSelect the most likely token at each step\n\t•\tDeterministic\n\t•\tFastest but often bland or locally optimal\n\n#### Random Sampling (LLM sampling)\n\t•\tSample from model distribution:\n\t•\tTemperature: smooth/sharp logits\n\t•\tTop-k: restrict to k highest-prob tokens\n\t•\tTop-p (nucleus): restrict to smallest set that covers p probability\n\t•\tProduces more diverse, creative outputs\n\n#### Best-of-N Sampling\n\t•\tGenerate N sampled continuations\n\t•\tCompute log-probability for each\n\t•\tReturn the continuation with highest average log-prob\n\t•\tImproves quality while keeping diversity\n\n#### Beam Search\n\t•\tKeep k candidate sequences (beams)\n\t•\tExpand each with all next tokens\n\t•\tRank by cumulative log-prob\n\t•\tKeep top-k for next step\n\t•\tProduces more coherent but less diverse text","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}