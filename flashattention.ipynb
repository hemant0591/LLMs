{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### FlashAttention — Summary\n\nFlashAttention is an IO-aware, exact attention algorithm that speeds up Transformer attention by reducing expensive GPU memory traffic.\nTraditional attention constructs and stores the full $N \\times N$ attention matrix in GPU DRAM (HBM), causing attention to become memory-bound.\n\n⸻\n\nKey Idea\n\nFlashAttention avoids materializing the full attention matrix.\nInstead, it:\n\t•\tSplits $Q$, $K$, $V$ into small tiles that fit in fast on-chip GPU SRAM\n\t•\tStreams tiles through SRAM instead of DRAM\n\t•\tComputes partial attention ($QK^\\top$ → softmax → softmax·$V$) on-chip\n\t•\tUses online softmax to merge partial results\n\t•\tWrites only the final output to DRAM\n\nThis reduces DRAM operations from $O(N^2)$ to $O(N \\cdot d)$, making attention dramatically faster.\n\n⸻\n\nWhy Online Softmax Works\n\nNormal softmax for a row:\n\n$$\n\\text{softmax}(s_i) = \\frac{e^{s_i}}{\\sum_j e^{s_j}}\n$$\n\nFlashAttention computes this incrementally by maintaining:\n\t•\tRunning max: $m$\n\t•\tRunning sum of exponentials: $l$\n\t•\tRunning output accumulator: $O$\n\nFor each tile:\n\t1.\tUpdate running max\n\t2.\tRescale old statistics\n\t3.\tAdd new exponentials\n\t4.\tAdd contribution from $V$ tile\n\nBecause softmax is invariant to subtracting a constant:\n\n$$\n\\frac{e^{s_i}}{\\sum_j e^{s_j}}\n$$\n\n$$\n\\frac{e^{s_i - m}}{\\sum_j e^{s_j - m}}\n$$\n\nthe online version produces exactly the same softmax as processing all scores at once.\n\n⸻\n\nFlashAttention-2 Improvements\n\nFlashAttention-2 improves performance via:\n\t•\tMore parallelism inside each head\n\t•\tWarp specialization (some warps load, others compute)\n\t•\tDouble-buffering K/V tiles\n\t•\tBetter Tensor Core utilization\n\t•\tFewer synchronizations\n\t•\tBetter handling of variable sequence lengths\n\nThis yields ~2× speedup over FA-1 (and 2–4× over naïve attention).\n\n⸻\n\nWhy It’s Faster\n\nOn-chip SRAM is:\n\t•\t10–20× faster than HBM\n\t•\t~100× lower latency\n\nFlashAttention keeps almost all intermediate computation in SRAM, only reading Q/K/V once and writing the final output once.\nThis removes the $N \\times N$ DRAM bottleneck and makes attention compute-bound, not memory-bound.\n\n⸻\n\nImplementation Overview\n\nFlashAttention kernels:\n\t•\tLoad Q/K/V tiles from HBM → SRAM\n\t•\tCompute $QK^\\top$ using Tensor Cores\n\t•\tApply masking + online softmax in SRAM\n\t•\tMultiply by V tiles\n\t•\tAccumulate outputs\n\t•\tWrite final output $O$ to DRAM\n\nThe $N \\times N$ matrix is never created.\n\nPyTorch 2+ automatically uses FlashAttention when calling:\nscaled_dot_product_attention(...)\n\non supported GPUs.\n\n⸻\n\nBenefits\n\t•\t2–4× faster attention\n\t•\t10× lower memory usage\n\t•\tEnables 8k–64k token contexts\n\t•\tExact (not approximate)\n\t•\tHelps both training and inference","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}