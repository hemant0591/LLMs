{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Speculative Decoding — Summary\n\nGoal: Speed up autoregressive generation by using a fast draft model to propose multiple next tokens and a large target model to validate them in one batched pass.\n\n#### Key idea:\n\t1.\tDraft model proposes a block of tokens quickly (e.g., 4–16 tokens).\n\t2.\tTarget model runs one forward pass over the context + proposed block (teacher-forced style) to compute logits at each position.\n\t3.\tIf the target model’s argmax at each step matches the draft token, accept those tokens; on the first mismatch, truncate at the mismatch and continue from there.\n\t4.\tRepeat: draft more → validate once → accept many.\n\n#### Why it’s faster:\n\t•\tVanilla decoding: 1 target-model forward pass per token.\n\t•\tSpeculative: 1 target-model forward pass per draft block (often 3–8 tokens accepted), cutting target passes by ~2–5× (depends on agreement and block size).\n\n#### Correctness:\n\t•\tThe large model still decides what gets accepted. No quality loss: accepted tokens are those the target model would have produced anyway.\n\n#### Where it helps:\n\t•\tGeneral chat/code models, RAG, long outputs.\n\t•\tLarger speedups when:\n\t•\tDraft is much faster than target,\n\t•\tAgreement rate is high,\n\t•\tBlock size is tuned to hardware.\n\nTypical speedups: ~2×–3× common, up to 4×–6× in favorable settings.\n\n#### KV caching (high-level):\n\t•\tKeep/extend the target KV cache only for accepted tokens.\n\t•\tThe draft model maintains its own KV while proposing.\n\t•\tValidation uses teacher forcing; you don’t commit target KV until acceptance.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"#### Algorithm Outline\n\t1.Given current context and target KV cache, loop until you reach max tokens:\n    \t•\tDraft step: Run the draft model autoregressively for M steps to propose tokens d_1…d_M (fast).\n    \t•\tValidate step (one pass): Feed context + d_1…d_M to the target model (teacher-forced) to get logits at each step.\n    \t•\tCompare: For each position t in 1…M, check if argmax(logits_t) == d_t.\n    \t•\tIf all match, accept all M; append to output; extend target KV accordingly.\n    \t•\tElse, accept up to the first mismatch k−1; append those; resume generation from there.\n\t2.Repeat until stop condition (EOS/max length).","metadata":{}},{"cell_type":"code","source":"# Pseudocode\n\nimport torch\n\n@torch.no_grad()\ndef speculative_decode(\n    target_model,         # large model (eval mode)\n    draft_model,          # small model (eval mode)\n    tokenizer,\n    prompt_ids: torch.LongTensor,   # [1, T_prompt]\n    max_new_tokens: int = 128,\n    draft_block: int = 8,           # how many tokens to propose per round\n    eos_id: int = None,\n):\n    \"\"\"\n    Returns generated token ids using speculative decoding.\n    \"\"\"\n    device = prompt_ids.device\n\n    # ---- Initialize state ----\n    out_ids = [*prompt_ids.squeeze(0).tolist()]  # running output (list of ints)\n\n    # Target KV cache for the *accepted* prefix only\n    tgt_kv = target_model.init_kv_cache(device=device)\n    # Prime the target KV with the prompt (teacher-forced)\n    tgt_logits, tgt_kv = target_model.forward_teacher_forced(\n        torch.tensor([out_ids], device=device), kv_cache=tgt_kv\n    )\n\n    # Draft KV cache (used only to propose tokens quickly)\n    draft_kv = draft_model.init_kv_cache(device=device)\n    _, draft_kv = draft_model.forward_teacher_forced(\n        torch.tensor([out_ids], device=device), kv_cache=draft_kv\n    )\n\n    # ---- Main loop ----\n    while len(out_ids) - prompt_ids.numel() < max_new_tokens:\n        # 1) DRAFT: propose up to draft_block tokens quickly\n        proposed = []\n        draft_ctx_ids = torch.tensor([[out_ids[-1]]], device=device)  # last token to continue\n        for _ in range(draft_block):\n            # one-step draft generation (autoregressive)\n            draft_logits, draft_kv = draft_model.generate_step(\n                draft_ctx_ids, kv_cache=draft_kv\n            )\n            next_id = int(draft_logits[:, -1].argmax(dim=-1))\n            proposed.append(next_id)\n            draft_ctx_ids = torch.tensor([[next_id]], device=device)\n\n            if eos_id is not None and next_id == eos_id:\n                break\n\n        if not proposed:\n            break\n\n        # 2) VALIDATE: single teacher-forced pass on target model over the proposed block\n        # Build the sequence [context + proposed]\n        seq_ids = out_ids + proposed\n        seq = torch.tensor([seq_ids], device=device)\n\n        # Teacher-forced forward over the *newly appended* positions only.\n        # Returns logits for each new step and an updated kv that *includes* the new steps.\n        # Implementation detail: you may pass only the tail window with kv to avoid redoing full prompt.\n        tgt_logits_block, tgt_kv_block = target_model.forward_teacher_forced_tail(\n            seq, prev_kv=tgt_kv, tail_len=len(proposed)\n        )\n        # tgt_logits_block shape: [1, len(proposed), vocab]\n\n        # 3) CHECK AGREEMENT position-by-position\n        accept_upto = 0\n        for t, token_id in enumerate(proposed):\n            # logits predicting token at position t (teacher-forced)\n            step_logits = tgt_logits_block[:, t, :]            # [1, V]\n            pred_id = int(step_logits.argmax(dim=-1))\n            if pred_id == token_id:\n                accept_upto += 1\n                if eos_id is not None and token_id == eos_id:\n                    break\n            else:\n                break\n\n        # 4) COMMIT accepted tokens (if any), update target KV accordingly\n        if accept_upto > 0:\n            # Commit only the accepted prefix of the block to out_ids\n            accepted = proposed[:accept_upto]\n            out_ids.extend(accepted)\n\n            # Commit target KV to the state *including* accepted tokens\n            tgt_kv = target_model.commit_kv_prefix(tgt_kv, tgt_kv_block, accept_upto)\n\n            # Also advance the draft KV by the same accepted tokens (so draft can continue efficiently)\n            draft_kv = draft_model.advance_kv(draft_kv, accepted)\n\n            # Early stop if EOS accepted\n            if eos_id is not None and accepted[-1] == eos_id:\n                break\n\n        # 5) If there was a mismatch, resume from the last accepted token\n        if accept_upto < len(proposed):\n            # We rejected the suffix (including the mismatched token).\n            # Now resume classic decoding on the target model for ONE token to break the tie.\n            # (Optionally: sample instead of greedy.)\n            tie_inp = torch.tensor([[out_ids[-1]]], device=device)\n            tie_logits, tgt_kv = target_model.generate_step(tie_inp, kv_cache=tgt_kv)\n            tie_id = int(tie_logits[:, -1].argmax(dim=-1))\n            out_ids.append(tie_id)\n\n            # Keep the draft model in sync after the tie token\n            _, draft_kv = draft_model.forward_teacher_forced(\n                torch.tensor([[tie_id]], device=device), kv_cache=draft_kv\n            )\n\n            if eos_id is not None and tie_id == eos_id:\n                break\n\n    return torch.tensor([out_ids], device=device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}