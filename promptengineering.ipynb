{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall -qqy jupyterlab  # Remove unused packages from Kaggle's base image that conflict\n!pip install -U -q \"google-genai==1.7.0\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:17:36.347220Z","iopub.execute_input":"2025-11-13T07:17:36.347594Z","iopub.status.idle":"2025-11-13T07:17:44.999587Z","shell.execute_reply.started":"2025-11-13T07:17:36.347567Z","shell.execute_reply":"2025-11-13T07:17:44.998420Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.18.0 requires google-genai<2.0.0,>=1.45.0, but you have google-genai 1.7.0 which is incompatible.\ngoogle-cloud-aiplatform 1.125.0 requires google-genai<2.0.0,>=1.37.0, but you have google-genai 1.7.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from google import genai\nfrom google.genai import types\n\nfrom IPython.display import HTML, Markdown, display, clear_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:17:45.001696Z","iopub.execute_input":"2025-11-13T07:17:45.001999Z","iopub.status.idle":"2025-11-13T07:17:46.634860Z","shell.execute_reply.started":"2025-11-13T07:17:45.001964Z","shell.execute_reply":"2025-11-13T07:17:46.633938Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"Set up a retry helper. This allows you to \"Run all\" without worrying about per-minute quota.","metadata":{}},{"cell_type":"code","source":"from google.api_core import retry\n\n\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\ngenai.models.Models.generate_content = retry.Retry(\n    predicate=is_retriable)(genai.models.Models.generate_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:17:46.635785Z","iopub.execute_input":"2025-11-13T07:17:46.636490Z","iopub.status.idle":"2025-11-13T07:17:48.963482Z","shell.execute_reply.started":"2025-11-13T07:17:46.636454Z","shell.execute_reply":"2025-11-13T07:17:48.962749Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:17:48.964354Z","iopub.execute_input":"2025-11-13T07:17:48.964813Z","iopub.status.idle":"2025-11-13T07:17:49.022080Z","shell.execute_reply.started":"2025-11-13T07:17:48.964788Z","shell.execute_reply":"2025-11-13T07:17:49.021247Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"#### Testing the model","metadata":{}},{"cell_type":"code","source":"client = genai.Client(api_key=GOOGLE_API_KEY)\n\nresponse = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=\"Explain AI to me like I'm a kid.\")\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:17:49.024440Z","iopub.execute_input":"2025-11-13T07:17:49.024767Z","iopub.status.idle":"2025-11-13T07:17:51.819324Z","shell.execute_reply.started":"2025-11-13T07:17:49.024744Z","shell.execute_reply":"2025-11-13T07:17:51.818391Z"}},"outputs":[{"name":"stdout","text":"Imagine you have a really, really smart puppy that can learn tricks. That puppy is like AI!\n\nInstead of learning tricks, AI learns from lots and lots of information. Think of it like showing the puppy a million pictures of cats so it knows exactly what a cat looks like.\n\nAI can do things like:\n\n*   **Recognize things:** Like knowing that's a dog in a picture, or understanding your voice when you ask it to play music.\n*   **Learn patterns:** Like figuring out what your favorite TV show is based on what you watch.\n*   **Make predictions:** Like guessing what word you're going to say next when you're typing a message.\n*   **Solve problems:** Like finding the shortest route on a map.\n\nIt's not magic, it's just really good at figuring things out based on all the information it's been shown. AI is getting smarter and smarter all the time! Sometimes it can even seem like it's thinking, but really it's just using what it's learned. It needs people to show it what to learn, and to help it learn correctly.\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"Markdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:17:51.820160Z","iopub.execute_input":"2025-11-13T07:17:51.820408Z","iopub.status.idle":"2025-11-13T07:17:51.828079Z","shell.execute_reply.started":"2025-11-13T07:17:51.820387Z","shell.execute_reply":"2025-11-13T07:17:51.827167Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Imagine you have a really, really smart puppy that can learn tricks. That puppy is like AI!\n\nInstead of learning tricks, AI learns from lots and lots of information. Think of it like showing the puppy a million pictures of cats so it knows exactly what a cat looks like.\n\nAI can do things like:\n\n*   **Recognize things:** Like knowing that's a dog in a picture, or understanding your voice when you ask it to play music.\n*   **Learn patterns:** Like figuring out what your favorite TV show is based on what you watch.\n*   **Make predictions:** Like guessing what word you're going to say next when you're typing a message.\n*   **Solve problems:** Like finding the shortest route on a map.\n\nIt's not magic, it's just really good at figuring things out based on all the information it's been shown. AI is getting smarter and smarter all the time! Sometimes it can even seem like it's thinking, but really it's just using what it's learned. It needs people to show it what to learn, and to help it learn correctly.\n"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"#### Start a chat","metadata":{}},{"cell_type":"code","source":"chat = client.chats.create(model='gemini-2.0-flash', history=[])\nresponse = chat.send_message('Hello! My name is Hemant.')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:17:51.828931Z","iopub.execute_input":"2025-11-13T07:17:51.829175Z","iopub.status.idle":"2025-11-13T07:17:52.502261Z","shell.execute_reply.started":"2025-11-13T07:17:51.829155Z","shell.execute_reply":"2025-11-13T07:17:52.501085Z"}},"outputs":[{"name":"stdout","text":"Hello Hemant! It's nice to meet you. How can I help you today?\n\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"response = chat.send_message('Can you tell me something interesting about black holes?')\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:17:52.502939Z","iopub.execute_input":"2025-11-13T07:17:52.503259Z","iopub.status.idle":"2025-11-13T07:17:55.175354Z","shell.execute_reply.started":"2025-11-13T07:17:52.503224Z","shell.execute_reply":"2025-11-13T07:17:55.174502Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Okay, here's something interesting about black holes that might blow your mind:\n\n**While black holes are known for sucking in everything, they also play a crucial role in the formation of new stars!**\n\nHere's the breakdown:\n\n*   **Accretion Disks and Jets:** Black holes are often surrounded by accretion disks – swirling masses of gas and dust pulled towards the black hole. As this material spirals inward, it heats up and releases tremendous amounts of energy. Some of this energy is expelled outwards in powerful jets of plasma that shoot out from the black hole's poles at near light speed.\n\n*   **Triggering Star Formation:** These jets, though incredibly powerful, can also compress surrounding clouds of gas and dust. This compression can provide the \"push\" needed for these clouds to overcome their internal pressure and collapse under their own gravity, initiating the process of star formation.\n\n*   **Controlling Star Formation:** While they can trigger it, black holes also regulate star formation. If the jets are *too* powerful, they can actually disrupt gas clouds and prevent star formation. This means that black holes act like cosmic regulators, preventing regions from becoming *too* dense with stars.\n\nSo, while black holes are notorious for their destructive nature, they also play a vital, and perhaps counterintuitive, role in the ongoing cycle of creation and destruction in the universe. They're not just cosmic vacuums; they're complex engines that influence the birth of new stars.\n\nIs there anything else you'd like to know about black holes or any other topic?\n"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"response = chat.send_message('Do you remember what my name is?')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:17:55.176262Z","iopub.execute_input":"2025-11-13T07:17:55.176514Z","iopub.status.idle":"2025-11-13T07:17:55.646336Z","shell.execute_reply.started":"2025-11-13T07:17:55.176493Z","shell.execute_reply":"2025-11-13T07:17:55.645552Z"}},"outputs":[{"name":"stdout","text":"Yes, your name is Hemant.\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"#### Available models","metadata":{}},{"cell_type":"code","source":"for model in client.models.list():\n  print(model.name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:17:55.647325Z","iopub.execute_input":"2025-11-13T07:17:55.647606Z","iopub.status.idle":"2025-11-13T07:17:55.828796Z","shell.execute_reply.started":"2025-11-13T07:17:55.647578Z","shell.execute_reply":"2025-11-13T07:17:55.827987Z"}},"outputs":[{"name":"stdout","text":"models/embedding-gecko-001\nmodels/gemini-2.5-pro-preview-03-25\nmodels/gemini-2.5-flash-preview-05-20\nmodels/gemini-2.5-flash\nmodels/gemini-2.5-flash-lite-preview-06-17\nmodels/gemini-2.5-pro-preview-05-06\nmodels/gemini-2.5-pro-preview-06-05\nmodels/gemini-2.5-pro\nmodels/gemini-2.0-flash-exp\nmodels/gemini-2.0-flash\nmodels/gemini-2.0-flash-001\nmodels/gemini-2.0-flash-exp-image-generation\nmodels/gemini-2.0-flash-lite-001\nmodels/gemini-2.0-flash-lite\nmodels/gemini-2.0-flash-preview-image-generation\nmodels/gemini-2.0-flash-lite-preview-02-05\nmodels/gemini-2.0-flash-lite-preview\nmodels/gemini-2.0-pro-exp\nmodels/gemini-2.0-pro-exp-02-05\nmodels/gemini-exp-1206\nmodels/gemini-2.0-flash-thinking-exp-01-21\nmodels/gemini-2.0-flash-thinking-exp\nmodels/gemini-2.0-flash-thinking-exp-1219\nmodels/gemini-2.5-flash-preview-tts\nmodels/gemini-2.5-pro-preview-tts\nmodels/learnlm-2.0-flash-experimental\nmodels/gemma-3-1b-it\nmodels/gemma-3-4b-it\nmodels/gemma-3-12b-it\nmodels/gemma-3-27b-it\nmodels/gemma-3n-e4b-it\nmodels/gemma-3n-e2b-it\nmodels/gemini-flash-latest\nmodels/gemini-flash-lite-latest\nmodels/gemini-pro-latest\nmodels/gemini-2.5-flash-lite\nmodels/gemini-2.5-flash-image-preview\nmodels/gemini-2.5-flash-image\nmodels/gemini-2.5-flash-preview-09-2025\nmodels/gemini-2.5-flash-lite-preview-09-2025\nmodels/gemini-robotics-er-1.5-preview\nmodels/gemini-2.5-computer-use-preview-10-2025\nmodels/embedding-001\nmodels/text-embedding-004\nmodels/gemini-embedding-exp-03-07\nmodels/gemini-embedding-exp\nmodels/gemini-embedding-001\nmodels/aqa\nmodels/imagen-4.0-generate-preview-06-06\nmodels/imagen-4.0-ultra-generate-preview-06-06\nmodels/imagen-4.0-generate-001\nmodels/imagen-4.0-ultra-generate-001\nmodels/imagen-4.0-fast-generate-001\nmodels/veo-2.0-generate-001\nmodels/veo-3.0-generate-001\nmodels/veo-3.0-fast-generate-001\nmodels/veo-3.1-generate-preview\nmodels/veo-3.1-fast-generate-preview\nmodels/gemini-2.0-flash-live-001\nmodels/gemini-live-2.5-flash-preview\nmodels/gemini-2.5-flash-live-preview\nmodels/gemini-2.5-flash-native-audio-latest\nmodels/gemini-2.5-flash-native-audio-preview-09-2025\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"#### Additional info about the model","metadata":{}},{"cell_type":"code","source":"from pprint import pprint\n\nfor model in client.models.list():\n  if model.name == 'models/gemini-2.0-flash':\n    pprint(model.to_json_dict())\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:17:55.829976Z","iopub.execute_input":"2025-11-13T07:17:55.830282Z","iopub.status.idle":"2025-11-13T07:17:55.907144Z","shell.execute_reply.started":"2025-11-13T07:17:55.830250Z","shell.execute_reply":"2025-11-13T07:17:55.906132Z"}},"outputs":[{"name":"stdout","text":"{'description': 'Gemini 2.0 Flash',\n 'display_name': 'Gemini 2.0 Flash',\n 'input_token_limit': 1048576,\n 'name': 'models/gemini-2.0-flash',\n 'output_token_limit': 8192,\n 'supported_actions': ['generateContent',\n                       'countTokens',\n                       'createCachedContent',\n                       'batchGenerateContent'],\n 'tuned_model_info': {},\n 'version': '2.0'}\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"#### Explore generation parameters","metadata":{}},{"cell_type":"code","source":"short_config = types.GenerateContentConfig(max_output_tokens=200)\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=short_config,\n    contents='Write 1000 word essay on modern corporate life and stress that comes with it.'\n)\n\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:17:55.908165Z","iopub.execute_input":"2025-11-13T07:17:55.908410Z","iopub.status.idle":"2025-11-13T07:17:57.999219Z","shell.execute_reply.started":"2025-11-13T07:17:55.908385Z","shell.execute_reply":"2025-11-13T07:17:57.998480Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"## The Gilded Cage: Stress and the Modern Corporate Life\n\nThe modern corporate landscape, a complex ecosystem of ambition, innovation, and cutthroat competition, has become synonymous with a particular type of stress. It's a stress that permeates not just the workplace but bleeds into the personal lives of its participants, shaping their habits, relationships, and overall well-being. While corporate life offers undeniable rewards – financial security, professional development, and a sense of purpose – it often comes at a steep cost: a relentless pressure to perform, adapt, and succeed in a world that is constantly evolving. This essay will explore the various facets of stress within the modern corporate world, examining the contributing factors, the individual and societal consequences, and ultimately, contemplating the need for a more sustainable and human-centric approach.\n\nOne of the most significant contributors to corporate stress is the **demanding work culture**. Driven by the relentless pursuit of productivity and profitability, companies often expect employees to be \"always on,\""},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"response = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=short_config,\n    contents='Write short poem on modern corporate life and stress that comes with it.'\n)\n\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:17:58.000375Z","iopub.execute_input":"2025-11-13T07:17:58.000798Z","iopub.status.idle":"2025-11-13T07:17:59.251959Z","shell.execute_reply.started":"2025-11-13T07:17:58.000766Z","shell.execute_reply":"2025-11-13T07:17:59.251125Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Sunrise paints the glass so high,\nAnother day to strive and lie.\nA thousand emails, urgent, stark,\nAnother meeting in the dark.\n\nThe screen glows, a hypnotic gleam,\nDistorting truth, a hollow dream.\nDeadlines loom, a crushing weight,\nAnother soul begins to break.\n\nSuccess defined in profit's name,\nErosion of a human frame.\nThe coffee's cold, the spirit frayed,\nAnother cog, forever swayed.\n"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"#### Playing with temperature","metadata":{}},{"cell_type":"code","source":"temp_config = types.GenerateContentConfig(temperature=2.0)\n\nfor _ in range(5):\n    response = client.models.generate_content(\n        model='gemini-2.0-flash',\n        config=temp_config,\n        contents='generate a color (respond with just one word)'\n    )\n\n    if response.text:\n        print(response.text)\n        print('#############')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:17:59.254986Z","iopub.execute_input":"2025-11-13T07:17:59.255265Z","iopub.status.idle":"2025-11-13T07:18:01.931576Z","shell.execute_reply.started":"2025-11-13T07:17:59.255241Z","shell.execute_reply":"2025-11-13T07:18:01.930486Z"}},"outputs":[{"name":"stdout","text":"Cerulean\n\n#############\nEmerald\n\n#############\nEmerald\n\n#############\nTeal\n\n#############\nTeal\n\n#############\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"temp_config = types.GenerateContentConfig(temperature=0.0)\n\nfor _ in range(5):\n    response = client.models.generate_content(\n        model='gemini-2.0-flash',\n        config=temp_config,\n        contents='generate a color (respond with just one word)'\n    )\n\n    if response.text:\n        print(response.text)\n        print('#############')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:18:01.932453Z","iopub.execute_input":"2025-11-13T07:18:01.932717Z","iopub.status.idle":"2025-11-13T07:18:04.207981Z","shell.execute_reply.started":"2025-11-13T07:18:01.932695Z","shell.execute_reply":"2025-11-13T07:18:04.207183Z"}},"outputs":[{"name":"stdout","text":"Azure\n\n#############\nAzure\n\n#############\nAzure\n\n#############\nAzure\n\n#############\nAzure\n\n#############\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"temp_config = types.GenerateContentConfig(temperature=0.2)\n\nfor _ in range(5):\n    response = client.models.generate_content(\n        model='gemini-2.0-flash',\n        config=temp_config,\n        contents='generate a color (respond with just one word)'\n    )\n\n    if response.text:\n        print(response.text)\n        print('#############')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:18:04.208980Z","iopub.execute_input":"2025-11-13T07:18:04.209284Z","iopub.status.idle":"2025-11-13T07:18:06.620666Z","shell.execute_reply.started":"2025-11-13T07:18:04.209252Z","shell.execute_reply":"2025-11-13T07:18:06.619701Z"}},"outputs":[{"name":"stdout","text":"Azure\n\n#############\nAzure\n\n#############\nAzure\n\n#############\nAzure\n\n#############\nAzure\n\n#############\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"model_config = types.GenerateContentConfig(\n    temperature=2.0,\n    top_p=0.95,\n)\n\nprompt = 'Write a story about a cat who quits his corporate job and retires to a peaceful life in the mountains'\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=model_config,\n    contents=prompt\n)\n\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:18:06.621613Z","iopub.execute_input":"2025-11-13T07:18:06.621925Z","iopub.status.idle":"2025-11-13T07:18:17.505452Z","shell.execute_reply.started":"2025-11-13T07:18:06.621898Z","shell.execute_reply":"2025-11-13T07:18:17.504426Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Bartholomew Buttons III, a ginger tabby of impeccable pedigree (if he did say so himself), sat at his desk. Or, more accurately, perched upon it. He glared at the spreadsheet projected on the wall – \"Q3 Revenue Projections: Premium Tuna Treats.\" The fluorescent lights hummed a relentless, soul-crushing buzz. He smoothed down his meticulously groomed fur, a gesture that had become a tic in the last few months.\n\nFor five years, Bartholomew had climbed the corporate ladder at \"Whiskers & Co.,\" purveyors of the finest feline cuisine. He had clawed his way from junior taste tester to Vice President of Tuna Innovation, a title he found vaguely embarrassing. But the constant pressure, the never-ending deadlines, the office politics disguised as innocent head-butts… it was all too much.\n\nHe swiveled his tail irritably. Suddenly, he saw himself – reflected in the sleek, stainless steel desk lamp – a corporate automaton. A cat devoid of joy. The realization hit him like a poorly aimed hairball.\n\n“That’s it!” he announced to the startled programmer a few desks over. He didn't bother waiting for a response. He was already unplugging his ergonomic, heat-controlled cat bed and grabbing his favorite laser pointer.\n\nThat afternoon, Bartholomew walked into Mr. Fluffernutter’s corner office – a mahogany mausoleum smelling faintly of catnip and despair – and slapped his letter of resignation (written in elegant paw script on parchment) onto the desk.\n\nMr. Fluffernutter, a portly Persian with a permanent air of dissatisfaction, barely glanced up. “Bartholomew, are you sure? This company has big plans for you. We were thinking of making you… Executive Vice President of Salmon Strategic Planning!”\n\nBartholomew shook his head. “I’m done, Mr. Fluffernutter. Done with the meetings, the memos, the constant chasing of the red dot… metaphorical and otherwise. I'm retiring.”\n\nHe paused, channeling his inner dramatic feline. \"I'm going to find myself. In the mountains.”\n\nHe spun on his heel and strode out, the squeak of his polished paws on the expensive rug echoing his freedom.\n\nThe mountains. He had always dreamed of them. Of pine-scented air, rustling leaves, and the absence of PowerPoint presentations. He sold his luxury penthouse apartment overlooking the tuna processing plant, packed a backpack with essentials – salmon jerky, a self-warming blanket, a very specific brand of catnip tea – and hopped on the next northbound train.\n\nHis mountain cabin was everything he’d hoped for. Cozy, rustic, and blessed with an abundance of sunbeams. He spent his days napping in hammocks strung between towering pines, chasing butterflies in meadows brimming with wildflowers, and perfecting the art of the purr.\n\nHe discovered hidden springs and drank from them, the water colder and fresher than any artificially flavored hydration concoction Whiskers & Co. had ever attempted. He learned to identify the tracks of local wildlife - the delicate prints of deer, the telltale signs of playful squirrels. He even made a friend, a grumpy badger named Bernard who grudgingly shared his berry patch.\n\nAt night, Bartholomew would sit on his porch, a mug of catnip tea steaming in his paws, and gaze at the star-dusted sky. The city’s incessant hum was replaced by the chirping of crickets and the hooting of owls. He felt a peace he had never known at Whiskers & Co.\n\nOne evening, Bernard shuffled up to the porch. “Hear you used to be… something fancy back in the city,” he grumbled, sniffing the air appreciatively. \"Tuna dude or something?\"\n\nBartholomew nodded. \"Vice President of Tuna Innovation.\"\n\nBernard snorted. \"Innovation, eh? Try innovatin' this,\" he said, and tossed a plump, juicy blackberry onto Bartholomew's lap. \"That's innovation, Button. Been around longer than your fancy tuna.\"\n\nBartholomew looked at the blackberry, its sweetness a promise on his tongue. He chuckled. Bernard, despite his gruff exterior, was right. He popped the berry into his mouth and closed his eyes, savoring the simple, natural flavor.\n\nHe had come to the mountains searching for peace and meaning. He found them not in spreadsheets and strategy meetings, but in the taste of wild berries, the warmth of the sun on his fur, and the quiet companionship of a grumpy old badger. He had finally, truly, retired. And for the first time in a long time, Bartholomew Buttons III felt content. His corporate days were over. His cat nap, on the other hand, was just beginning.\n"},"metadata":{}}],"execution_count":17},{"cell_type":"markdown","source":"#### Prompting","metadata":{}},{"cell_type":"markdown","source":"##### Zero shot prompt","metadata":{}},{"cell_type":"code","source":"model_config = types.GenerateContentConfig(\n    temperature=0.1,\n    top_p=1,\n    max_output_tokens=5\n)\n\nzero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\nReview: \"Her\" is a disturbing study revealing the direction\nhumanity is headed if AI is allowed to keep evolving,\nunchecked. I wish there were more movies like this masterpiece.\nSentiment: \"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=model_config,\n    contents=zero_shot_prompt\n)\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:18:17.506397Z","iopub.execute_input":"2025-11-13T07:18:17.506717Z","iopub.status.idle":"2025-11-13T07:18:17.917609Z","shell.execute_reply.started":"2025-11-13T07:18:17.506689Z","shell.execute_reply":"2025-11-13T07:18:17.916293Z"}},"outputs":[{"name":"stdout","text":"POSITIVE\n\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"### Enum mode","metadata":{}},{"cell_type":"code","source":"import enum\n\nclass Sentiment(enum.Enum):\n    POSITIVE= 'pos'\n    NEGETIVE= 'neg'\n    NEUTRAL= 'neu'\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=types.GenerateContentConfig(\n        response_mime_type=\"text/x.enum\",\n        response_schema=Sentiment\n    ),\n    contents=zero_shot_prompt\n)\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:18:17.918740Z","iopub.execute_input":"2025-11-13T07:18:17.919004Z","iopub.status.idle":"2025-11-13T07:18:18.463981Z","shell.execute_reply.started":"2025-11-13T07:18:17.918981Z","shell.execute_reply":"2025-11-13T07:18:18.463053Z"}},"outputs":[{"name":"stdout","text":"pos\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"enum_response = response.parsed\nprint(enum_response)\nprint(type(enum_response))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:18:18.464857Z","iopub.execute_input":"2025-11-13T07:18:18.465108Z","iopub.status.idle":"2025-11-13T07:18:18.470162Z","shell.execute_reply.started":"2025-11-13T07:18:18.465086Z","shell.execute_reply":"2025-11-13T07:18:18.469068Z"}},"outputs":[{"name":"stdout","text":"Sentiment.POSITIVE\n<enum 'Sentiment'>\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"#### Example workflow","metadata":{}},{"cell_type":"code","source":"from google.genai.types import Part\n\nclass CustomerIntent(enum.Enum):\n    CANCEL_ORDER = 'cancel_order'\n    CHECK_STATUS = 'check_status'\n    REQUEST_REFUND = 'request_refund'\n    GENERAL_QUERY = 'general_query'\n    PRODUCT_INFO = 'product_info'\n\nSYSTEM_PROMPT = (\n    \"You are an expert intent classifier for a retail company. \"\n    \"Your task is to analyze the user's message and determine the single, primary intent. \"\n    \"You MUST only respond with one of the provided enum values.\"\n)\n\ndef classify_intent(user_message):\n    contents = [\n        # Put the system prompt as a user-role content (server accepts \"user\")\n        types.Content(\n            role=\"user\",\n            parts=[Part.from_text(text=SYSTEM_PROMPT)]\n        ),\n        # Actual user message\n        types.Content(\n            role=\"user\",\n            parts=[Part.from_text(text=user_message)]\n        ),\n    ]\n\n\n    try:\n        response = client.models.generate_content(\n            model='gemini-2.5-flash',\n            contents=contents, \n            config=types.GenerateContentConfig(\n                response_mime_type=\"text/x.enum\",\n                response_schema=CustomerIntent\n            )\n        )\n        return response.text.strip()\n    \n    except Exception as e:\n        return f\"API Error: {e}\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:18:18.471068Z","iopub.execute_input":"2025-11-13T07:18:18.471538Z","iopub.status.idle":"2025-11-13T07:18:18.490301Z","shell.execute_reply.started":"2025-11-13T07:18:18.471459Z","shell.execute_reply":"2025-11-13T07:18:18.489440Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"message_1 = \"I need to get my money back because the item arrived broken.\"\nintent_1 = classify_intent(message_1)\nprint(intent_1)\n\nmessage_2 = \"Can you tell me where my package is right now? I ordered it last week.\"\nintent_2 = classify_intent(message_2)\n\nmessage_3 = \"Does the new X-Pro 500 come in red, and what are the main features?\"\nintent_3 = classify_intent(message_3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:18:18.491380Z","iopub.execute_input":"2025-11-13T07:18:18.491705Z","iopub.status.idle":"2025-11-13T07:18:20.436730Z","shell.execute_reply.started":"2025-11-13T07:18:18.491675Z","shell.execute_reply":"2025-11-13T07:18:20.435711Z"}},"outputs":[{"name":"stdout","text":"request_refund\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"def trigger_workflow(intent: str, message: str):\n    \"\"\"Simulates a backend routing workflow based on the classified intent.\"\"\"\n    \n    print(f\"\\nUser Message: '{message}'\")\n    print(f\"Classified Intent: {intent}\")\n    \n    match intent:\n        case CustomerIntent.REQUEST_REFUND.value:\n            print(\"Routing: Initiating Refund Request Workflow.\")\n        case CustomerIntent.CHECK_STATUS.value:\n            print(\"Routing: Querying Shipping Database for Order Status.\")\n        case CustomerIntent.CANCEL_ORDER.value:\n            print(\"Routing: Sending Order Cancellation Confirmation.\")\n        case CustomerIntent.PRODUCT_INFO.value:\n            print(\"Routing: Connecting to Product Knowledge Base.\")\n        case _:\n            # This should not happen if the enum enforcement works\n            print(\"ERROR: Received an invalid intent. Escalating to human review.\")\n\n\n# Run the workflow simulation\ntrigger_workflow(intent_1, message_1)\ntrigger_workflow(intent_2, message_2)\ntrigger_workflow(intent_3, message_3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:18:20.437655Z","iopub.execute_input":"2025-11-13T07:18:20.438073Z","iopub.status.idle":"2025-11-13T07:18:20.445789Z","shell.execute_reply.started":"2025-11-13T07:18:20.438040Z","shell.execute_reply":"2025-11-13T07:18:20.444679Z"}},"outputs":[{"name":"stdout","text":"\nUser Message: 'I need to get my money back because the item arrived broken.'\nClassified Intent: request_refund\nRouting: Initiating Refund Request Workflow.\n\nUser Message: 'Can you tell me where my package is right now? I ordered it last week.'\nClassified Intent: check_status\nRouting: Querying Shipping Database for Order Status.\n\nUser Message: 'Does the new X-Pro 500 come in red, and what are the main features?'\nClassified Intent: product_info\nRouting: Connecting to Product Knowledge Base.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"#### Few shot prompt","metadata":{}},{"cell_type":"code","source":"few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n\nEXAMPLE:\nI want a small pizza with cheese, tomato sauce, and pepperoni.\nJSON Response:\n```\n{\n\"size\": \"small\",\n\"type\": \"normal\",\n\"ingredients\": [\"cheese\", \"tomato sauce\", \"pepperoni\"]\n}\n```\n\nEXAMPLE:\nCan I get a large pizza with tomato sauce, basil and mozzarella\nJSON Response:\n```\n{\n\"size\": \"large\",\n\"type\": \"normal\",\n\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n}\n```\n\nORDER:\n\"\"\"\n\ncustomer_order = \"Give me a large with half margherita & half burrata salad\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=types.GenerateContentConfig(\n        temperature=0.1,\n        top_p=1,\n        max_output_tokens=250,\n    ),\n    contents=[few_shot_prompt, customer_order])\n\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:18:20.446914Z","iopub.execute_input":"2025-11-13T07:18:20.447825Z","iopub.status.idle":"2025-11-13T07:18:21.686146Z","shell.execute_reply.started":"2025-11-13T07:18:20.447798Z","shell.execute_reply":"2025-11-13T07:18:21.685193Z"}},"outputs":[{"name":"stdout","text":"```json\n{\n  \"size\": \"large\",\n  \"type\": \"half and half\",\n  \"ingredients_1\": [\"tomato sauce\", \"mozzarella\", \"basil\"],\n  \"ingredients_2\": [\"burrata\", \"salad\"]\n}\n```\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"#### JSON output","metadata":{}},{"cell_type":"code","source":"import typing_extensions as typing\n\nclass PizzaOrder(typing.TypedDict):\n    size: str\n    ingredients: list[str]\n    type: str\n\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=types.GenerateContentConfig(\n        temperature=0.1,\n        response_mime_type=\"application/json\",\n        response_schema=PizzaOrder,\n    ),\n    contents=\"Can I have a large margherita pizza?\")\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:18:21.687150Z","iopub.execute_input":"2025-11-13T07:18:21.687450Z","iopub.status.idle":"2025-11-13T07:18:22.567808Z","shell.execute_reply.started":"2025-11-13T07:18:21.687419Z","shell.execute_reply":"2025-11-13T07:18:22.566912Z"}},"outputs":[{"name":"stdout","text":"{\n  \"size\": \"large\",\n  \"ingredients\": [\"tomato sauce\", \"mozzarella cheese\", \"basil\"],\n  \"type\": \"margherita\"\n}\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"#### Exploring CoT","metadata":{}},{"cell_type":"code","source":"prompt = \"\"\"When I was 8 years old, my sister was half my age. Now, I\nam 34 years old. How old is my sister? Return the answer directly.\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    contents=prompt)\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:18:22.568766Z","iopub.execute_input":"2025-11-13T07:18:22.569045Z","iopub.status.idle":"2025-11-13T07:18:23.068788Z","shell.execute_reply.started":"2025-11-13T07:18:22.569022Z","shell.execute_reply":"2025-11-13T07:18:23.067892Z"}},"outputs":[{"name":"stdout","text":"30\n\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\nam 20 years old. How old is my partner? Return the answer directly.\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    contents=prompt)\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:18:23.069680Z","iopub.execute_input":"2025-11-13T07:18:23.069946Z","iopub.status.idle":"2025-11-13T07:18:23.495921Z","shell.execute_reply.started":"2025-11-13T07:18:23.069924Z","shell.execute_reply":"2025-11-13T07:18:23.494923Z"}},"outputs":[{"name":"stdout","text":"48\n\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\nI am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    contents=prompt)\n\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:18:23.499390Z","iopub.execute_input":"2025-11-13T07:18:23.499709Z","iopub.status.idle":"2025-11-13T07:18:24.850594Z","shell.execute_reply.started":"2025-11-13T07:18:23.499678Z","shell.execute_reply":"2025-11-13T07:18:24.849762Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Here's how to solve the problem step-by-step:\n\n1. **Find the age difference:** When you were 4, your partner was 3 times your age, meaning they were 4 * 3 = 12 years old.\n\n2. **Calculate the age difference:** The age difference between you and your partner is 12 - 4 = 8 years.\n\n3. **Determine your partner's current age:** Since the age difference remains constant, your partner is always 8 years older than you. Now that you are 20, your partner is 20 + 8 = 28 years old.\n\n**Answer:** Your partner is currently 28 years old.\n"},"metadata":{}}],"execution_count":28},{"cell_type":"markdown","source":"#### Exploring ReAct","metadata":{}},{"cell_type":"code","source":"model_instructions = \"\"\"\nSolve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\nObservation is understanding relevant information from an Action's output and Action can be one of three types:\n (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n     will return some similar entities to search and you can try to search the information from those topics.\n (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n     so keep your searches short.\n (3) <finish>answer</finish>, which returns the answer and finishes the task.\n\"\"\"\n\nexample1 = \"\"\"Question\nMusician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n\nThought 1\nThe question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n\nAction 1\n<search>Milhouse</search>\n\nObservation 1\nMilhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n\nThought 2\nThe paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n\nAction 2\n<lookup>named after</lookup>\n\nObservation 2\nMilhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n\nThought 3\nMilhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n\nAction 3\n<finish>Richard Nixon</finish>\n\"\"\"\n\nexample2 = \"\"\"Question\nWhat is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n\nThought 1\nI need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n\nAction 1\n<search>Colorado orogeny</search>\n\nObservation 1\nThe Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n\nThought 2\nIt does not mention the eastern sector. So I need to look up eastern sector.\n\nAction 2\n<lookup>eastern sector</lookup>\n\nObservation 2\nThe eastern sector extends into the High Plains and is called the Central Plains orogeny.\n\nThought 3\nThe eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n\nAction 3\n<search>High Plains</search>\n\nObservation 3\nHigh Plains refers to one of two distinct land regions\n\nThought 4\nI need to instead search High Plains (United States).\n\nAction 4\n<search>High Plains (United States)</search>\n\nObservation 4\nThe High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n\nThought 5\nHigh Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n\nAction 5\n<finish>1,800 to 7,000 ft</finish>\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:18:24.851445Z","iopub.execute_input":"2025-11-13T07:18:24.851699Z","iopub.status.idle":"2025-11-13T07:18:24.857625Z","shell.execute_reply.started":"2025-11-13T07:18:24.851672Z","shell.execute_reply":"2025-11-13T07:18:24.856700Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"question = \"\"\"Question\nWho was the youngest author listed on the transformers NLP paper?\n\"\"\"\n\nreact_config = types.GenerateContentConfig(\n    stop_sequences=[\"\\nObservation\"],\n    system_instruction=model_instructions + example1 + example2,\n)\n\n\nreact_chat = client.chats.create(\n    model='gemini-2.0-flash',\n    config=react_config,\n)\n\nresp = react_chat.send_message(question)\nprint(resp.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:18:24.858789Z","iopub.execute_input":"2025-11-13T07:18:24.859116Z","iopub.status.idle":"2025-11-13T07:18:25.720735Z","shell.execute_reply.started":"2025-11-13T07:18:24.859088Z","shell.execute_reply":"2025-11-13T07:18:25.719849Z"}},"outputs":[{"name":"stdout","text":"Thought 1\nI need to find the transformers NLP paper and then identify the youngest author listed on the paper.\n\nAction 1\n<search>transformers NLP paper</search>\n\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"observation = \"\"\"Observation 1\n[1706.03762] Attention Is All You Need\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\nWe propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n\"\"\"\nresp = react_chat.send_message(observation)\nprint(resp.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:18:25.722045Z","iopub.execute_input":"2025-11-13T07:18:25.722303Z","iopub.status.idle":"2025-11-13T07:18:26.999418Z","shell.execute_reply.started":"2025-11-13T07:18:25.722273Z","shell.execute_reply":"2025-11-13T07:18:26.998484Z"}},"outputs":[{"name":"stdout","text":"Thought 2\nThe paper is Attention Is All You Need. The authors listed are Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. I need to find the youngest author of these authors. Since I cannot directly look up the age of the authors, I will start with the first author and search the author.\n\nAction 2\n<search>Ashish Vaswani</search>\n\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"#### Thinking mode (using experimental thinking model)\n\nThe experiemental Gemini Flash 2.0 \"Thinking\" model has been trained to generate the \"thinking process\" the model goes through as part of its response. As a result, the Flash Thinking model is capable of stronger reasoning capabilities in its responses.\n\nUsing a \"thinking mode\" model can provide you with high-quality responses without needing specialised prompting like the previous approaches. One reason this technique is effective is that you induce the model to generate relevant information (\"brainstorming\", or \"thoughts\") that is then used as part of the context in which the final response is generated.","metadata":{}},{"cell_type":"code","source":"import io\n\nresponse = client.models.generate_content_stream(\n    model='gemini-2.0-flash-thinking-exp',\n    contents='Who was the youngest author listed on the transformers NLP paper?',\n)\n\nbuf = io.StringIO()\nfor chunk in response:\n    buf.write(chunk.text)\n    # Display the response as it is streamed\n    print(chunk.text, end='')\n\n# And then render the finished response as formatted markdown.\nclear_output()\nMarkdown(buf.getvalue())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:20:06.360156Z","iopub.execute_input":"2025-11-13T07:20:06.360496Z","iopub.status.idle":"2025-11-13T07:20:11.322161Z","shell.execute_reply.started":"2025-11-13T07:20:06.360473Z","shell.execute_reply":"2025-11-13T07:20:11.321404Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"The youngest author listed on the \"Attention Is All You Need\" paper (which introduced the Transformer architecture, often referred to as the Transformers NLP paper) was **Aidan N. Gomez**.\n\nHe was 22 years old when the paper was published in 2017."},"metadata":{}}],"execution_count":32},{"cell_type":"markdown","source":"#### Code generation","metadata":{}},{"cell_type":"code","source":"code_prompt = \"\"\"\nWrite a Python function to calculate the factorial of a number. No explanation, provide only the code.\n\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=types.GenerateContentConfig(\n        temperature=1.0,\n        top_p=1.0,\n        max_output_tokens=1000\n    ),\n    contents=code_prompt\n)\n\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:23:36.846003Z","iopub.execute_input":"2025-11-13T07:23:36.846967Z","iopub.status.idle":"2025-11-13T07:23:38.030107Z","shell.execute_reply.started":"2025-11-13T07:23:36.846935Z","shell.execute_reply":"2025-11-13T07:23:38.028983Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"```python\ndef factorial(n):\n  \"\"\"\n  Calculate the factorial of a number.\n\n  Args:\n    n: An integer.\n\n  Returns:\n    The factorial of n.\n  \"\"\"\n  if n == 0:\n    return 1\n  else:\n    return n * factorial(n-1)\n```\n"},"metadata":{}}],"execution_count":35},{"cell_type":"markdown","source":"#### Code execution","metadata":{}},{"cell_type":"code","source":"config = types.GenerateContentConfig(\n    tools=[types.Tool(code_execution=types.ToolCodeExecution())]\n)\n\ncode_exec_prompt = \"\"\"\nGenerate the first 14 odd prime numbers, then calculate their sum.\n\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=config,\n    contents=code_exec_prompt)\n\nresponse","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:34:12.217710Z","iopub.execute_input":"2025-11-13T07:34:12.218267Z","iopub.status.idle":"2025-11-13T07:34:14.915263Z","shell.execute_reply.started":"2025-11-13T07:34:12.218169Z","shell.execute_reply":"2025-11-13T07:34:14.914358Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"Okay, I understand. First, I need to generate the first 14 odd prime numbers. Remember that a prime number is a number greater than 1 that has only two divisors: 1 and itself. The first few primes are 2, 3, 5, 7, 11, etc. Since I need *odd* prime numbers, I will exclude 2 from the list. Then, I'll calculate their sum.\\n\\n\"), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=ExecutableCode(code=\"primes = [3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\\nsum_of_primes = sum(primes)\\nprint(f'{primes=}')\\nprint(f'{sum_of_primes=}')\\n\", language=<Language.PYTHON: 'PYTHON'>), file_data=None, function_call=None, function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=CodeExecutionResult(outcome=<Outcome.OUTCOME_OK: 'OUTCOME_OK'>, output='primes=[3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\\nsum_of_primes=326\\n'), executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='The first 14 odd prime numbers are 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, and 47. Their sum is 326.\\n')], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=None, finish_reason=<FinishReason.STOP: 'STOP'>, grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)], create_time=None, response_id=None, model_version='gemini-2.0-flash', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=318, prompt_token_count=17, total_token_count=598), automatic_function_calling_history=[], parsed=None)"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"for part in response.candidates[0].content.parts:\n    pprint(part.to_json_dict())\n    print(\"-----\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:38:21.521480Z","iopub.execute_input":"2025-11-13T07:38:21.521795Z","iopub.status.idle":"2025-11-13T07:38:21.528162Z","shell.execute_reply.started":"2025-11-13T07:38:21.521769Z","shell.execute_reply":"2025-11-13T07:38:21.527088Z"}},"outputs":[{"name":"stdout","text":"{'text': 'Okay, I understand. First, I need to generate the first 14 odd prime '\n         'numbers. Remember that a prime number is a number greater than 1 '\n         'that has only two divisors: 1 and itself. The first few primes are '\n         '2, 3, 5, 7, 11, etc. Since I need *odd* prime numbers, I will '\n         \"exclude 2 from the list. Then, I'll calculate their sum.\\n\"\n         '\\n'}\n-----\n{'executable_code': {'code': 'primes = [3, 5, 7, 11, 13, 17, 19, 23, 29, 31, '\n                             '37, 41, 43, 47]\\n'\n                             'sum_of_primes = sum(primes)\\n'\n                             \"print(f'{primes=}')\\n\"\n                             \"print(f'{sum_of_primes=}')\\n\",\n                     'language': 'PYTHON'}}\n-----\n{'code_execution_result': {'outcome': 'OUTCOME_OK',\n                           'output': 'primes=[3, 5, 7, 11, 13, 17, 19, 23, 29, '\n                                     '31, 37, 41, 43, 47]\\n'\n                                     'sum_of_primes=326\\n'}}\n-----\n{'text': 'The first 14 odd prime numbers are 3, 5, 7, 11, 13, 17, 19, 23, 29, '\n         '31, 37, 41, 43, and 47. Their sum is 326.\\n'}\n-----\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"for part in response.candidates[0].content.parts:\n    if part.text:\n        display(Markdown(part.text))\n    elif part.executable_code:\n        display(Markdown(f'```python\\n{part.executable_code.code}\\n```'))\n    elif part.code_execution_result:\n        if part.code_execution_result.outcome != 'OUTCOME_OK':\n            display(Markdown(f'## Status {part.code_execution_result.outcome}'))\n\n        display(Markdown(f'```\\n{part.code_execution_result.output}\\n```'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:39:16.406729Z","iopub.execute_input":"2025-11-13T07:39:16.407069Z","iopub.status.idle":"2025-11-13T07:39:16.418505Z","shell.execute_reply.started":"2025-11-13T07:39:16.407043Z","shell.execute_reply":"2025-11-13T07:39:16.417533Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Okay, I understand. First, I need to generate the first 14 odd prime numbers. Remember that a prime number is a number greater than 1 that has only two divisors: 1 and itself. The first few primes are 2, 3, 5, 7, 11, etc. Since I need *odd* prime numbers, I will exclude 2 from the list. Then, I'll calculate their sum.\n\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"```python\nprimes = [3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\nsum_of_primes = sum(primes)\nprint(f'{primes=}')\nprint(f'{sum_of_primes=}')\n\n```"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"```\nprimes=[3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\nsum_of_primes=326\n\n```"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"The first 14 odd prime numbers are 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, and 47. Their sum is 326.\n"},"metadata":{}}],"execution_count":53},{"cell_type":"markdown","source":"#### Explain code using Gemini","metadata":{}},{"cell_type":"code","source":"file_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh\n\nexplain_prompt = f\"\"\"\nPlease explain what this file does at a very high level. What is it, and why would I use it?\n\n```\n{file_contents}\n```\n\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    contents=explain_prompt)\n\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T07:40:18.076673Z","iopub.execute_input":"2025-11-13T07:40:18.077675Z","iopub.status.idle":"2025-11-13T07:40:21.691130Z","shell.execute_reply.started":"2025-11-13T07:40:18.077644Z","shell.execute_reply":"2025-11-13T07:40:21.690183Z"}},"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"This file is a **Bash script that enhances your command-line prompt to display Git repository status information**.  In other words, it makes your prompt aware of Git.\n\nHere's a breakdown:\n\n*   **What it is:** It's a collection of shell functions and configuration designed to be sourced (included) in your `.bashrc` (or equivalent) file. When sourced, it modifies how your command prompt looks.\n\n*   **What it does:**  The core function, `setGitPrompt` (and related helper functions), figures out if the current directory is part of a Git repository. If it is, it extracts information like the current branch, the state of the working directory (modified, staged, untracked files), and the relationship with remote branches (ahead, behind).  It then formats this information and adds it to your prompt using color-coded symbols.\n\n*   **Why use it?**\n    *   **Git Awareness:**  Quickly see the status of your Git repository without having to run `git status` manually.\n    *   **Improved Workflow:** Makes it easier to manage your Git repositories by keeping you informed about changes and branch information at a glance.\n    *   **Customization:** Allows you to customize the colors, symbols, and information displayed in the prompt to suit your preferences.\n    *   **Virtual Environment Awareness**: Can be configured to display your currently active virtual environment.\n\nIn short, this script provides a visually informative and convenient way to keep track of your Git repositories directly in your command-line prompt.  Instead of a plain prompt like `user@host:~$`, you might see something like `user@host:~/my-repo (main↓2↑1*)` indicating you are on the `main` branch, 2 commits behind and 1 commit ahead of the remote, with some local changes.\n"},"metadata":{}}],"execution_count":54},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}