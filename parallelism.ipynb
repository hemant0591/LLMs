{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### 1. Sequence Parallelism\n\nGoal: Split work along the sequence dimension (tokens) so that each GPU holds different tokens of the same batch.\n\n#### How it works\n\t•\tThe input sequence (e.g., 2048 tokens) is divided across GPUs.\n\t•\tGPU 0 processes tokens 0–1023\n\t•\tGPU 1 processes tokens 1024–2047\n\t•\tEach GPU computes its part of attention, feed-forward, layer norm, etc.\n\t•\tRequired cross-GPU operations:\n\t•\tAll-gather of hidden states\n\t•\tReduce-scatter for attention outputs\n\t•\tSharded softmax, sharded dropout, etc.\n\n#### Why it helps\n\t•\tReduces activation memory per GPU (biggest memory saver for long sequences).\n\t•\tEspecially useful for long-context LLMs.\n\t•\tCompatible with tensor parallelism.\n\nAnalogy:\nEach GPU handles “its slice” of the sequence like chapters in a book, but they sync before/after attention.\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"### 2. Pipeline Parallelism\n\nGoal: Split the model layers across GPUs.\n\nExample: A 48-layer transformer across 4 GPUs:\n\n\t•\tGPU 0 → Layers 1–12\n\t•\tGPU 1 → Layers 13–24\n\t•\tGPU 2 → Layers 25–36\n\t•\tGPU 3 → Layers 37–48\n\nHow it works\n\nForward pass flows like a pipeline:\nBatch → GPU0 → GPU1 → GPU2 → GPU3 → loss\n\nBackward pass flows in reverse.\n\nTo avoid idle GPUs, we use microbatching:\n\nMicrobatch 1 in GPU0  \nMicrobatch 2 in GPU0 while Microbatch 1 is in GPU1  \n...\n\nWhy it helps\n\t•\tAllows training of very deep networks that don’t fit on a single GPU.\n\t•\tGood for vertical model partitioning.\n\nCost\n\t•\tPipeline “bubble” (idle time) unless microbatching is tuned well.\n\t•\tAdds communication cost between stages.\n\nAnalogy\n\nAn assembly line: each GPU handles a segment of the model.","metadata":{}},{"cell_type":"markdown","source":"### 3. Tensor Parallelism (Model Parallelism)\n\nGoal: Split individual layers (matrices) across GPUs.\n\nExample: a linear layer\n\nY = XW\n\nwhere W is too large for a single GPU.\n\nSplit W:\n\n\t•\tHorizontal split: shard output features\n\t•\tVertical split: shard input features\n\nHow it works\n\nGPU 0 computes part of XW\nGPU 1 computes part of XW\nGPU 2 computes part …\nThen reduce-sum or concat to form the full output.\n\nUsed for:\n\n\t•\tLarge feed-forward layers\n\t•\tLarge attention projections (Q/K/V, output matrices)\n\nWhy it helps\n\n\t•\tAllows a single layer with billions of parameters to run across multiple GPUs.\n\t•\tRequired for modern 70B–500B+ parameter models.\n\nCost\n\n\t•\tHeavy all-reduce communications.\n\t•\tNeeds high-speed interconnect (NVLink, H100 NVSwitch).\n\nAnalogy:\nEach GPU holds part of a giant matrix. They all compute partial results and merge them.","metadata":{}},{"cell_type":"markdown","source":"### How They Fit Together\n\nModern large-scale training uses all three:\n\nTensor Parallelism → lets each layer fit in GPU memory\n\nPipeline Parallelism → lets all layers fit across GPUs\n\nSequence Parallelism → keeps activation memory manageable\n\nMegatron-LM, DeepSpeed, PaLM, Llama, GPT-4 training stacks use a combination of these.","metadata":{}},{"cell_type":"markdown","source":"### Tensor Parallelism (shard big matrices across GPUs)\n\nIdea: split large Linear/attention projections across a tensor group; each GPU computes a partial result → all-reduce/concat to form the full output.","metadata":{}},{"cell_type":"code","source":"# Assuming we have 8 GPUs\n# global ranks: 0 1 2 3 4 5 6 7\n# TP Group 0: [0, 1, 2, 3]\n# TP Group 1: [4, 5, 6, 7]\n# tp_ranks = [0, 1, 2, 3]       # for the first group\n# tp_ranks = [4, 5, 6, 7]       # for the second group\n\n\n# world: N GPUs\n# create a tensor-parallel group tp_group (subset of ranks), size = TP\ntp_group = dist.new_group(ranks=tp_ranks)\n\nclass TensorParallelLinear(nn.Module):\n    def __init__(self, in_features, out_features, tp_group):\n        super().__init__()\n        self.tp_group = tp_group\n        self.tp_size = dist.get_world_size(tp_group)\n\n        # column-parallel shard: split OUT features across GPUs\n        assert out_features % self.tp_size == 0\n        out_local = out_features // self.tp_size\n        self.weight = nn.Parameter(torch.empty(out_local, in_features))\n        self.bias   = nn.Parameter(torch.empty(out_local))\n\n        # init weights (e.g., xavier) … and place on local device\n        ...\n\n    def forward(self, x):\n        # x: [B, *, in_features], replicated across tp_group\n        # local matmul -> partial output shard\n        y_local = F.linear(x, self.weight, self.bias)  # [B, *, out_local]\n        # to assemble full output, all-gather shards along last dim\n        y_list = [torch.empty_like(y_local) for _ in range(self.tp_size)]\n        dist.all_gather(y_list, y_local, group=self.tp_group)\n        y_full = torch.cat(y_list, dim=-1)            # [B, *, out_features]\n        return y_full\n\nclass TensorParallelRowLinear(nn.Module):\n    \"\"\"Row-parallel shard: split IN features, then all-reduce partial outputs.\"\"\"\n    def __init__(self, in_features, out_features, tp_group):\n        super().__init__()\n        self.tp_group = tp_group\n        self.tp_size  = dist.get_world_size(tp_group)\n        assert in_features % self.tp_size == 0\n        in_local = in_features // self.tp_size\n        self.weight = nn.Parameter(torch.empty(out_features, in_local))\n        self.bias   = nn.Parameter(torch.empty(out_features))\n        ...\n\n    def forward(self, x_sharded):\n        # x_sharded is pre-split across tp_group: [B, *, in_local]\n        # each rank computes partial matmul -> [B, *, out_features]\n        y_partial = F.linear(x_sharded, self.weight)  # no bias yet\n        # sum partials across ranks (all-reduce)\n        dist.all_reduce(y_partial, op=dist.ReduceOp.SUM, group=self.tp_group)\n        y_full = y_partial + self.bias\n        return y_full\n\n# In attention, do the same for QKV and out-proj:\n# - QKV: column-parallel (gather after attention)\n# - out-proj: row-parallel (all-reduce)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\t•\tChoose column-parallel for projections that are later concatenated; row-parallel for projections that are summed.\n\t•\tNeeds fast interconnect (NVLink/NVSwitch).","metadata":{}},{"cell_type":"markdown","source":"### Pipeline Parallelism (split layers into stages across GPUs)\n\nIdea: partition the model by layers into P stages on different ranks; feed microbatches through a 1F1B (one-forward-one-backward) schedule to reduce bubbles.","metadata":{}},{"cell_type":"code","source":"# Suppose world size = P * DP (ignore TP here for clarity)\n# Define stage_id per rank, create p2p connections with next/prev stage.\nstage_id = ...           # 0 .. P-1\nprev_rank = rank-1 if stage_id > 0 else None\nnext_rank = rank+1 if stage_id < P-1 else None\n\n# Split model layers\nfull_layers = build_transformer_layers(L)\npartitions = split_layers(full_layers, P)        # list of layer-slices\nstage = nn.Sequential(*partitions[stage_id]).to(local_device)\n\ndef run_pipeline_step(microbatch):\n    \"\"\"One microbatch forward on this stage.\"\"\"\n    if stage_id == 0:\n        x = microbatch.to(local_device)          # source stage reads data\n    else:\n        x = recv_tensor(src=prev_rank)\n\n    # Forward through local layers\n    y = stage(x)\n\n    if stage_id == P-1:\n        # compute loss/logits; return to host or start backward\n        logits = y\n        return logits\n    else:\n        send_tensor(y, dst=next_rank)\n\ndef train_one_iteration(microbatches):\n    \"\"\"Naive 1F1B sketch; real schedulers overlap F/B + p2p.\"\"\"\n    # warmup forwards to fill the pipe\n    for mb in microbatches[:stage_id]:\n        run_pipeline_step(mb)\n\n    # steady state: for each mb, do F; receive grad; do B; send grad back\n    for i, mb in enumerate(microbatches[stage_id: stage_id + (len(microbatches) - (P-1 - stage_id))]):\n        # FORWARD\n        y = run_pipeline_step(mb)\n\n        # receive grad from next stage (except last)\n        if stage_id < P-1:\n            grad_y = recv_tensor(src=next_rank)\n            y.backward(grad_y)\n        else:\n            # last stage computes loss and starts backward\n            loss = loss_fn(y, labels_for_mb(i))\n            loss.backward()\n            # send grad to prev\n            send_tensor(y.grad, dst=prev_rank)\n\n    # drain the pipe: remaining backward passes\n    ...\n\n# send/recv utilities (pseudocode)\ndef send_tensor(t, dst):  dist.send(t.detach(), dst=dst)\ndef recv_tensor(src):     buf = torch.empty_like(proto_tensor); dist.recv(buf, src=src); return buf","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\t•\tReal implementations use torch.distributed.pipeline (or DeepSpeed/Megatron schedulers), overlapping p2p comms with compute and finely tuning microbatch size to minimize bubbles.","metadata":{}},{"cell_type":"markdown","source":"### Sequence Parallelism (shard tokens across GPUs inside a TP group)\n\nIdea: split the sequence length across ranks (e.g., 4 GPUs each hold 1/4 of the tokens). Use all-gather/reduce-scatter around places that need full-sequence context (attention), and keep per-token ops local.","metadata":{}},{"cell_type":"code","source":"# Assume we already have a tensor-parallel group tp_group of size = SP (sequence-parallel size)\nsp_group = tp_group\nrank_in_sp = dist.get_rank(sp_group)\nsp_size    = dist.get_world_size(sp_group)\n\ndef split_sequence(x):\n    # x: [B, S, H], shard S across ranks\n    assert S % sp_size == 0\n    S_local = S // sp_size\n    start = rank_in_sp * S_local\n    end   = start + S_local\n    return x[:, start:end, :]  # [B, S_local, H]\n\ndef allgather_sequence(x_local):\n    # gather shards along S to recover [B, S, H]\n    bufs = [torch.empty_like(x_local) for _ in range(sp_size)]\n    dist.all_gather(bufs, x_local, group=sp_group)\n    return torch.cat(bufs, dim=1)\n\ndef reducescatter_sequence(y_full):\n    # inverse of all-gather after ops that produce [B,S,H]\n    # split y_full along S then sum (or just split if concatenate semantics)\n    chunks = list(y_full.chunk(sp_size, dim=1))\n    y_local = torch.empty_like(chunks[0])\n    dist.reduce_scatter(y_local, chunks, op=dist.ReduceOp.SUM, group=sp_group)\n    return y_local\n\nclass SeqParallelBlock(nn.Module):\n    def __init__(self, attn, mlp):\n        super().__init__()\n        self.attn = attn    # standard attention submodule\n        self.mlp  = mlp\n\n    def forward(self, h_full):\n        # h_full: [B,S,H] (typically comes from previous layer’s gather or the input)\n        # 1) shard tokens across ranks\n        h_local = split_sequence(h_full)            # [B,S_local,H]\n\n        # 2) per-token ops stay local (e.g., LayerNorm on last dim)\n        h_local = layernorm(h_local)\n\n        # 3) attention needs global context across S:\n        #    gather all tokens -> run attention -> scatter back\n        h_all = allgather_sequence(h_local)         # [B,S,H]\n        a_all = self.attn(h_all)                    # [B,S,H]\n        a_local = reducescatter_sequence(a_all)     # [B,S_local,H]\n\n        # 4) MLP is position-wise → can be local if implemented with TP-friendly shards\n        #    (or do gather->mlp->scatter similarly, depending on layout)\n        m_local = self.mlp(a_local)                 # [B,S_local,H]\n\n        return m_local\n\n# Stacking blocks:\n#   for each transformer layer within a TP group, alternate split/gather/scatter as needed.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\t•\tSequence parallelism is mainly a training trick to reduce activation memory for long sequences.\n\t•\tFor inference decoding, we usually avoid SP because it hurts KV-cache locality and prefix reuse.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}