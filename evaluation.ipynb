{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -Uq \"google-genai==1.7.0\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:21:33.680539Z","iopub.execute_input":"2025-11-14T09:21:33.680850Z","iopub.status.idle":"2025-11-14T09:21:39.168429Z","shell.execute_reply.started":"2025-11-14T09:21:33.680825Z","shell.execute_reply":"2025-11-14T09:21:39.167606Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.18.0 requires google-genai<2.0.0,>=1.45.0, but you have google-genai 1.7.0 which is incompatible.\ngoogle-cloud-aiplatform 1.125.0 requires google-genai<2.0.0,>=1.37.0, but you have google-genai 1.7.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from google import genai\nfrom google.genai import types\n\nfrom IPython.display import Markdown, display\n\ngenai.__version__","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:21:39.170079Z","iopub.execute_input":"2025-11-14T09:21:39.170356Z","iopub.status.idle":"2025-11-14T09:21:40.664264Z","shell.execute_reply.started":"2025-11-14T09:21:39.170325Z","shell.execute_reply":"2025-11-14T09:21:40.663569Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'1.7.0'"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nclient = genai.Client(api_key=UserSecretsClient().get_secret(\"GOOGLE_API_KEY\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:21:40.665103Z","iopub.execute_input":"2025-11-14T09:21:40.665619Z","iopub.status.idle":"2025-11-14T09:21:41.158177Z","shell.execute_reply.started":"2025-11-14T09:21:40.665587Z","shell.execute_reply":"2025-11-14T09:21:41.157521Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from google.api_core import retry\n\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\nif not hasattr(genai.models.Models.generate_content, '__wrapped__'):\n    genai.models.Models.generate_content = retry.Retry(predicate=is_retriable)(genai.models.Models.generate_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:21:41.160191Z","iopub.execute_input":"2025-11-14T09:21:41.160528Z","iopub.status.idle":"2025-11-14T09:21:43.099214Z","shell.execute_reply.started":"2025-11-14T09:21:41.160480Z","shell.execute_reply":"2025-11-14T09:21:43.098532Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### Evaluation\n\nWe'll evaluate a summarisation task using the Gemini 1.5 Pro technical report.","metadata":{}},{"cell_type":"code","source":"!wget -nv -O gemini.pdf https://storage.googleapis.com/cloud-samples-data/generative-ai/pdf/2403.05530.pdf\n\ndocument_file = client.files.upload(file='gemini.pdf')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:21:43.099989Z","iopub.execute_input":"2025-11-14T09:21:43.100390Z","iopub.status.idle":"2025-11-14T09:21:46.034678Z","shell.execute_reply.started":"2025-11-14T09:21:43.100360Z","shell.execute_reply":"2025-11-14T09:21:46.033734Z"}},"outputs":[{"name":"stdout","text":"2025-11-14 09:21:44 URL:https://storage.googleapis.com/cloud-samples-data/generative-ai/pdf/2403.05530.pdf [7228817/7228817] -> \"gemini.pdf\" [1]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"#### Summarise a document\nThe summarisation request used here is fairly basic. It targets the training content specifically but provides no guidance otherwise.","metadata":{}},{"cell_type":"code","source":"request = 'Tell me about the training process used here.'\n\ndef summarize_doc(request):\n    \"\"\"Execute the request on the uploaded document.\"\"\"\n    config = types.GenerateContentConfig(temperature=0.0)\n\n    response = client.models.generate_content(\n        model='gemini-2.0-flash',\n        config=config,\n        contents=[request, document_file]\n    )\n\n    return response.text\n\nsummary = summarize_doc(request)\nMarkdown(summary)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:21:46.035859Z","iopub.execute_input":"2025-11-14T09:21:46.036119Z","iopub.status.idle":"2025-11-14T09:21:58.896843Z","shell.execute_reply.started":"2025-11-14T09:21:46.036092Z","shell.execute_reply":"2025-11-14T09:21:58.895979Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Based on the document you provided, here's a breakdown of the training process used for Gemini 1.5 Pro:\n\n**1. Data:**\n\n*   **Multimodal and Multilingual Data:** The model is trained on a diverse dataset that includes text, images, audio, and video content. The text data is sourced from various domains, including web documents and code.\n*   **Pre-training Dataset:** The pre-training dataset includes data sourced across many different domains, including web documents and code, and incorporates image, audio, and video content.\n*   **Instruction-Tuning Phase:** Gemini 1.5 Pro is fine-tuned on a collection of multimodal data containing paired instructions and appropriate responses, with further tuning based on human preference data.\n\n**2. Architecture:**\n\n*   **Mixture-of-Experts (MoE) Transformer:** Gemini 1.5 Pro is based on a sparse MoE Transformer architecture. This allows the model to have a large number of parameters while only activating a subset for any given input.\n\n**3. Infrastructure:**\n\n*   **TPUv4 Accelerators:** The model is trained on multiple 4096-chip pods of Google's TPUv4 accelerators, distributed across multiple datacenters.\n\n**4. Training Process:**\n\n*   **Pre-training:** The model is initially pre-trained on the large multimodal dataset.\n*   **Instruction Tuning:** After pre-training, the model is fine-tuned on a collection of multimodal data containing paired instructions and appropriate responses.\n*   **Human Preference Tuning:** Further tuning is performed based on human preference data.\n\n**5. Key Improvements:**\n\n*   **Architecture:** Improvements across the model stack, including architecture, data, optimization, and systems.\n*   **Long-Context Understanding:** Significant architecture changes enable understanding of inputs up to 10 million tokens without performance degradation.\n\n**In summary:** Gemini 1.5 Pro is trained using a large, diverse multimodal dataset on Google's TPUv4 infrastructure. It uses a MoE Transformer architecture and undergoes pre-training, instruction tuning, and human preference tuning. The training process incorporates improvements across the model stack to enable long-context understanding and overall performance."},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"#### Define an evaluator\n\nFor a task like this, we want to evaluate a number of aspects, like how well the model followed the prompt (\"instruction following\"), whether it included relevant data in the prompt (\"groundedness\"), how easy the text is to read (\"fluency\"), or other factors like \"verbosity\" or \"quality\".\n\nIn this step, we define an evaluation agent using a pre-written \"summarisation\" prompt and use it to gauge the quality of the generated summary.","metadata":{}},{"cell_type":"code","source":"SUMMARY_PROMPT = \"\"\"\\\n# Instruction\nYou are an expert evaluator. Your task is to evaluate the quality of the responses generated by AI models.\nWe will provide you with the user input and an AI-generated responses.\nYou should first read the user input carefully for analyzing the task, and then evaluate the quality of the responses based on the Criteria provided in the Evaluation section below.\nYou will assign the response a rating following the Rating Rubric and Evaluation Steps. Give step-by-step explanations for your rating, and only choose ratings from the Rating Rubric.\n\n# Evaluation\n## Metric Definition\nYou will be assessing summarization quality, which measures the overall ability to summarize text. Pay special attention to length constraints, such as in X words or in Y sentences. The instruction for performing a summarization task and the context to be summarized are provided in the user prompt. The response should be shorter than the text in the context. The response should not contain information that is not present in the context.\n\n## Criteria\nInstruction following: The response demonstrates a clear understanding of the summarization task instructions, satisfying all of the instruction's requirements.\nGroundedness: The response contains information included only in the context. The response does not reference any outside information.\nConciseness: The response summarizes the relevant details in the original text without a significant loss in key information without being too verbose or terse.\nFluency: The response is well-organized and easy to read.\n\n## Rating Rubric\n5: (Very good). The summary follows instructions, is grounded, is concise, and fluent.\n4: (Good). The summary follows instructions, is grounded, concise, and fluent.\n3: (Ok). The summary mostly follows instructions, is grounded, but is not very concise and is not fluent.\n2: (Bad). The summary is grounded, but does not follow the instructions.\n1: (Very bad). The summary is not grounded.\n\n## Evaluation Steps\nSTEP 1: Assess the response in aspects of instruction following, groundedness, conciseness, and verbosity according to the criteria.\nSTEP 2: Score based on the rubric.\n\n# User Inputs and AI-generated Response\n## User Inputs\n\n### Prompt\n{prompt}\n\n## AI-generated Response\n{response}\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:21:58.897703Z","iopub.execute_input":"2025-11-14T09:21:58.897926Z","iopub.status.idle":"2025-11-14T09:21:58.903145Z","shell.execute_reply.started":"2025-11-14T09:21:58.897906Z","shell.execute_reply":"2025-11-14T09:21:58.902270Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import enum\n\n# Define a structured enum class to capture the result.\nclass SummaryRating(enum.Enum):\n  VERY_GOOD = '5'\n  GOOD = '4'\n  OK = '3'\n  BAD = '2'\n  VERY_BAD = '1'\n\ndef eval_summary(prompt, ai_response):\n    \"\"\"Evaluate the generated summary against the prompt used.\"\"\"\n\n    chat = client.chats.create(model='gemini-2.0-flash')\n\n    #generate full chat response\n    response = chat.send_message(\n        message=SUMMARY_PROMPT.format(prompt=prompt, response=ai_response)\n    )\n\n    verbose_eval = response.text\n\n    # get desired structure\n    structured_output_config = types.GenerateContentConfig(\n        response_mime_type=\"text/x.enum\",\n        response_schema=SummaryRating,\n        )\n\n    response = chat.send_message(\n      message=\"Convert the final score.\",\n      config=structured_output_config,\n      )\n    structured_eval = response.parsed\n\n    return verbose_eval, structured_eval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:21:58.904023Z","iopub.execute_input":"2025-11-14T09:21:58.904276Z","iopub.status.idle":"2025-11-14T09:21:58.926337Z","shell.execute_reply.started":"2025-11-14T09:21:58.904251Z","shell.execute_reply":"2025-11-14T09:21:58.925551Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"text_eval, struct_eval = eval_summary(prompt=[request, document_file], ai_response=summary)\nMarkdown(text_eval)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:21:58.927251Z","iopub.execute_input":"2025-11-14T09:21:58.927578Z","iopub.status.idle":"2025-11-14T09:22:01.994232Z","shell.execute_reply.started":"2025-11-14T09:21:58.927556Z","shell.execute_reply":"2025-11-14T09:22:01.993557Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"## Evaluation\nSTEP 1:\nThe AI-generated response summarizes the training process of Gemini 1.5 Pro based on the uploaded document, covering aspects like data, architecture, infrastructure, and training stages. The information is grounded in the document, as it accurately reflects the details provided about the multimodal dataset, MoE Transformer architecture, TPUv4 accelerators, and the pre-training, instruction tuning, and human preference tuning stages. The response could be more concise. The level of detail is good, but it could be condensed slightly to focus on the most critical aspects without losing key information. The response is generally fluent and well-organized, making it easy to understand. The use of bullet points and clear headings helps structure the information effectively.\n\nSTEP 2:\nThe response fulfills the instructions, is grounded, and is mostly concise and fluent.\nI will give a score of 4.\n\n## Rating:\n4"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"struct_eval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:22:01.996554Z","iopub.execute_input":"2025-11-14T09:22:01.996789Z","iopub.status.idle":"2025-11-14T09:22:02.002079Z","shell.execute_reply.started":"2025-11-14T09:22:01.996770Z","shell.execute_reply":"2025-11-14T09:22:02.001270Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"<SummaryRating.GOOD: '4'>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"new_prompt = \"Explain like I'm 5 the training process\"\n\nif not new_prompt:\n  raise ValueError(\"Try setting a new summarisation prompt.\")\n\ndef run_and_eval_summary(prompt):\n  \"\"\"Generate and evaluate the summary using the new prompt.\"\"\"\n  summary = summarize_doc(new_prompt)\n  display(Markdown(summary + '\\n-----'))\n\n  text, struct = eval_summary([new_prompt, document_file], summary)\n  display(Markdown(text + '\\n-----'))\n  print(struct)\n\nrun_and_eval_summary(new_prompt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:22:02.002914Z","iopub.execute_input":"2025-11-14T09:22:02.003136Z","iopub.status.idle":"2025-11-14T09:22:16.557183Z","shell.execute_reply.started":"2025-11-14T09:22:02.003116Z","shell.execute_reply":"2025-11-14T09:22:16.556326Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Okay, I can explain the training process of a large language model like Gemini 1.5 Pro in a way that a 5-year-old can understand.\n\nImagine you have a puppy, and you want to teach it to understand and respond to your commands. That's kind of like training a big computer brain!\n\n1.  **Lots of Examples:** First, you show the puppy lots and lots of things. You show it pictures of cats, dogs, cars, and houses. You also read it stories and tell it about all sorts of things. The computer brain also gets to see and read lots of things – millions and millions of pictures, books, and websites!\n\n2.  **Learning Patterns:** The puppy starts to notice patterns. It learns that things with pointy ears and a tail are often dogs, and that when you say \"sit,\" it should put its bottom on the ground. The computer brain also learns patterns. It learns that certain words often go together, and that certain pictures are related to certain words.\n\n3.  **Making Predictions:** Now, you ask the puppy a question, like \"Where's the ball?\" The puppy tries to guess where the ball is based on what it has learned. The computer brain also tries to guess the answer to questions.\n\n4.  **Getting Feedback:** If the puppy guesses right, you give it a treat and say \"Good job!\" If it guesses wrong, you gently correct it. The computer brain also gets feedback. If it guesses right, it gets a little reward. If it guesses wrong, it adjusts itself to try to guess better next time.\n\n5.  **Repeating and Improving:** You keep showing the puppy things, asking questions, and giving feedback over and over again. The puppy gets better and better at understanding and responding to you. The computer brain also keeps learning and improving. It gets better at understanding and answering questions, and even at doing new things that it wasn't specifically taught!\n\nSo, training a big computer brain is like teaching a puppy, but with lots and lots of examples, and instead of treats, the computer brain gets little rewards that help it learn and improve. And just like a well-trained puppy, a well-trained computer brain can be very helpful and do amazing things!\n-----"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"## Evaluation\nSTEP 1: \nThe response is well-written and easy to understand, and it is written as though speaking to a child of 5 years old. The response is grounded in the document that was provided.\n\nSTEP 2:\n4 - The response is good, and does follow instructions. The explanation is very good for the prompt. grounded, concise, and fluent.\n-----"},"metadata":{}},{"name":"stdout","text":"SummaryRating.GOOD\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"#### Pointwise evaluation\n\nThe technique used above, where we evaluated a single input/output pair against some criteria is known as pointwise evaluation. This is useful for evaluating singular outputs in an absolute sense, such as \"was it good or bad?\"\n\nIn this exercise, we will try different guidance prompts with a set of questions.","metadata":{}},{"cell_type":"code","source":"terse_guidance = \"Answer the following question in a single sentence, or as close to that as possible.\"\nmoderate_guidance = \"Provide a brief answer to the following question, use a citation if necessary, but only enough to answer the question.\"\ncited_guidance = \"Provide a thorough, detailed answer to the following question, citing the document and supplying additional background information as much as possible.\"\nguidance_options = {\n    'Terse': terse_guidance,\n    'Moderate': moderate_guidance,\n    'Cited': cited_guidance,\n}\n\nquestions = [\n    \"What metric(s) are used to evaluate long context performance?\",\n    \"How does the model perform on code tasks?\",\n    \"How many layers does it have?\",\n    # \"Why is it called Gemini?\",\n]\n\nif not questions:\n  raise NotImplementedError('Add some questions to evaluate!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:22:16.558157Z","iopub.execute_input":"2025-11-14T09:22:16.558446Z","iopub.status.idle":"2025-11-14T09:22:16.563240Z","shell.execute_reply.started":"2025-11-14T09:22:16.558418Z","shell.execute_reply":"2025-11-14T09:22:16.562548Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import functools\n\n@functools.cache # caches the output of function in LRU cache (makes fetching results later easy O(1))\ndef answer_question(question, guidance):\n    \"\"\"Generate an answer to the question using the uploaded document and guidance.\"\"\"\n    config = types.GenerateContentConfig(\n        temperature=0.0,\n        system_instruction=guidance\n    )\n\n    response = client.models.generate_content(\n        model='gemini-2.0-flash',\n        config=config,\n        contents=[question, document_file]\n    )\n\n    return response.text\n\nanswer = answer_question(questions[0], terse_guidance)\nMarkdown(answer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:22:16.564113Z","iopub.execute_input":"2025-11-14T09:22:16.564354Z","iopub.status.idle":"2025-11-14T09:22:26.124418Z","shell.execute_reply.started":"2025-11-14T09:22:16.564329Z","shell.execute_reply":"2025-11-14T09:22:26.123706Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Long context performance is evaluated using metrics such as near-perfect recall on retrieval tasks, improvements in long-document QA, long-video QA, and long-context ASR, and by matching or surpassing the performance of other models on a broad set of benchmarks.\n"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"answer = answer_question(questions[0], cited_guidance)\nMarkdown(answer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:22:26.125295Z","iopub.execute_input":"2025-11-14T09:22:26.125566Z","iopub.status.idle":"2025-11-14T09:22:39.686662Z","shell.execute_reply.started":"2025-11-14T09:22:26.125538Z","shell.execute_reply":"2025-11-14T09:22:39.685901Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Based on the document \"Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context\", here's a breakdown of the metrics used to evaluate long context performance:\n\n**1. Diagnostic Long-Context Evaluations:**\n\n*   **Perplexity over Long Sequences:**\n    *   **Metric:** Negative Log-Likelihood (NLL) of tokens at different positions in the input sequences.\n    *   **Interpretation:** A lower NLL indicates better next-token prediction and more effective use of long-context information. The trend of the NLL curve (downward or upward) reveals whether the model is effectively using the context or deteriorating in prediction quality.\n*   **Needle-in-a-Haystack Retrieval:**\n    *   **Metric:** Recall rate (percentage of successful retrievals).\n    *   **Task:** The model is given a long context (the \"haystack\") and asked to find a specific piece of information (the \"needle\") inserted at a random location.\n    *   **Modalities:** This task is performed across text, video, and audio.\n    *   **Variations:** The document also explores variations of this task, such as retrieving multiple needles in a single turn and modulating retrieval difficulty by varying the similarity of the needles.\n\n**2. Realistic Long-Context Evaluations:**\n\n*   **In-Context Language Learning (Machine Translation from One Book - MTOB):**\n    *   **Task:** Learning to translate a new language (Kalamang) from a single set of linguistic documentation (grammar, dictionary, parallel sentences).\n    *   **Metrics:**\n        *   Human evaluation scores (on a scale of 0 to 6, with 6 being an excellent translation).\n        *   Automatic metrics: BLEURT (for Kalamang to English) and chrF (for English to Kalamang).\n*   **Long-Document Question Answering:**\n    *   **Task:** Answering questions about a long document (e.g., \"Les Misérables\").\n    *   **Metric:** Human evaluation using the Attributable to Identified Sources (AIS) protocol.\n    *   **Evaluation:** The model's ability to answer questions correctly when the entire document is provided as input.\n*   **Long-Context Automatic Speech Recognition (ASR):**\n    *   **Task:** Transcribing long audio segments (15-minute videos).\n    *   **Metric:** Word Error Rate (WER).\n*   **Long-Context Video Question Answering:**\n    *   **Task:** Answering questions about long videos (40-105 minutes).\n    *   **Metric:** Accuracy.\n\n**Additional Context and Background:**\n\n*   **Importance of Long Context:** The document emphasizes that expanding the context window allows models to incorporate more task-specific information not found in the training data, leading to improved performance.\n*   **Comparison with Other Models:** Gemini 1.5 Pro is compared against Gemini 1.0 Pro/Ultra, Claude 2.1, and GPT-4 Turbo on various tasks to demonstrate its capabilities.\n*   **Trade-offs:** The document also addresses the potential trade-offs between long-context performance and core capabilities (performance on non-long-context tasks).\n*   **Responsible Deployment:** The document includes a section on responsible deployment, covering impact assessment, model mitigations, and safety evaluations.\n*   **Divergence:** The document also evaluates Gemini 1.5 Pro to understand its susceptibility to divergence and in particular, emitting memorized training data via this attack.\n"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"Now let's set up a question-answering evaluator, much like before, but using the pointwise QA evaluation prompt.","metadata":{}},{"cell_type":"code","source":"QA_PROMPT = \"\"\"\\\n# Instruction\nYou are an expert evaluator. Your task is to evaluate the quality of the responses generated by AI models.\nWe will provide you with the user prompt and an AI-generated responses.\nYou should first read the user prompt carefully for analyzing the task, and then evaluate the quality of the responses based on and rules provided in the Evaluation section below.\n\n# Evaluation\n## Metric Definition\nYou will be assessing question answering quality, which measures the overall quality of the answer to the question in the user prompt. Pay special attention to length constraints, such as in X words or in Y sentences. The instruction for performing a question-answering task is provided in the user prompt. The response should not contain information that is not present in the context (if it is provided).\n\nYou will assign the writing response a score from 5, 4, 3, 2, 1, following the Rating Rubric and Evaluation Steps.\nGive step-by-step explanations for your scoring, and only choose scores from 5, 4, 3, 2, 1.\n\n## Criteria Definition\nInstruction following: The response demonstrates a clear understanding of the question answering task instructions, satisfying all of the instruction's requirements.\nGroundedness: The response contains information included only in the context if the context is present in the user prompt. The response does not reference any outside information.\nCompleteness: The response completely answers the question with sufficient detail.\nFluent: The response is well-organized and easy to read.\n\n## Rating Rubric\n5: (Very good). The answer follows instructions, is grounded, complete, and fluent.\n4: (Good). The answer follows instructions, is grounded, complete, but is not very fluent.\n3: (Ok). The answer mostly follows instructions, is grounded, answers the question partially and is not very fluent.\n2: (Bad). The answer does not follow the instructions very well, is incomplete or not fully grounded.\n1: (Very bad). The answer does not follow the instructions, is wrong and not grounded.\n\n## Evaluation Steps\nSTEP 1: Assess the response in aspects of instruction following, groundedness,completeness, and fluency according to the criteria.\nSTEP 2: Score based on the rubric.\n\n# User Inputs and AI-generated Response\n## User Inputs\n### Prompt\n{prompt}\n\n## AI-generated Response\n{response}\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:22:39.687466Z","iopub.execute_input":"2025-11-14T09:22:39.687732Z","iopub.status.idle":"2025-11-14T09:22:39.692675Z","shell.execute_reply.started":"2025-11-14T09:22:39.687708Z","shell.execute_reply":"2025-11-14T09:22:39.691823Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class AnswerRating(enum.Enum):\n  VERY_GOOD = '5'\n  GOOD = '4'\n  OK = '3'\n  BAD = '2'\n  VERY_BAD = '1'\n\n\n@functools.cache\ndef eval_answer(prompt, ai_response, n=1):\n    \"\"\"Evaluate the generated answer against the prompt/question used.\"\"\"\n    chat = client.chats.create(model='gemini-2.0-flash')\n\n    response = chat.send_message(\n        message=QA_PROMPT.format(prompt=prompt, response=ai_response)\n    )\n    verbose_eval = response.text\n\n    # structured output\n    config = types.GenerateContentConfig(\n        response_mime_type=\"text/x.enum\",\n        response_schema=AnswerRating,\n    )\n\n    response = chat.send_message(\n      message=\"Convert the final score.\",\n      config=config,\n      )\n    structured_eval = response.parsed\n\n    return verbose_eval, structured_eval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:22:39.693508Z","iopub.execute_input":"2025-11-14T09:22:39.693781Z","iopub.status.idle":"2025-11-14T09:22:39.716720Z","shell.execute_reply.started":"2025-11-14T09:22:39.693756Z","shell.execute_reply":"2025-11-14T09:22:39.715829Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"text_eval, struct_eval = eval_answer(prompt=questions[0], ai_response=answer)\ndisplay(Markdown(text_eval))\nprint(struct_eval)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:22:39.717652Z","iopub.execute_input":"2025-11-14T09:22:39.717937Z","iopub.status.idle":"2025-11-14T09:22:41.672014Z","shell.execute_reply.started":"2025-11-14T09:22:39.717910Z","shell.execute_reply":"2025-11-14T09:22:41.671226Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"STEP 1: Assess the response in aspects of instruction following, groundedness, completeness, and fluency according to the criteria.\n\nThe response is grounded, as it states that the information comes from the document \"Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context\". The answer completely answers the question with sufficient detail. The response is well-organized and easy to read. The response demonstrates a clear understanding of the question answering task instructions, satisfying all of the instruction's requirements.\n\nSTEP 2: Score based on the rubric.\n\n5\n"},"metadata":{}},{"name":"stdout","text":"AnswerRating.VERY_GOOD\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"struct_eval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:22:41.672842Z","iopub.execute_input":"2025-11-14T09:22:41.673082Z","iopub.status.idle":"2025-11-14T09:22:41.678460Z","shell.execute_reply.started":"2025-11-14T09:22:41.673058Z","shell.execute_reply":"2025-11-14T09:22:41.677643Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"<AnswerRating.VERY_GOOD: '5'>"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"import collections\nimport itertools\n\nNUM_ITERATIONS = 1\n\nscores = collections.defaultdict(int)\nresponses = collections.defaultdict(list)\n\nfor question in questions:\n    display(Markdown(f'## {question}'))\n    for guidance, guide_prompt in guidance_options.items():\n        for n in range(NUM_ITERATIONS):\n            answer = answer_question(question, guide_prompt)\n            written_eval, struct_eval = eval_answer(question, answer, n)\n            print(f'{guidance}: {struct_eval}')\n            scores[guidance] += int(struct_eval.value)\n            responses[(guidance, question)].append((answer, written_eval))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:22:41.679379Z","iopub.execute_input":"2025-11-14T09:22:41.679651Z","iopub.status.idle":"2025-11-14T09:24:41.374832Z","shell.execute_reply.started":"2025-11-14T09:22:41.679624Z","shell.execute_reply":"2025-11-14T09:24:41.373985Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"## What metric(s) are used to evaluate long context performance?"},"metadata":{}},{"name":"stdout","text":"Terse: AnswerRating.VERY_GOOD\nModerate: AnswerRating.VERY_GOOD\nCited: AnswerRating.VERY_GOOD\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"## How does the model perform on code tasks?"},"metadata":{}},{"name":"stdout","text":"Terse: AnswerRating.VERY_GOOD\nModerate: AnswerRating.GOOD\nCited: AnswerRating.VERY_GOOD\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"## How many layers does it have?"},"metadata":{}},{"name":"stdout","text":"Terse: AnswerRating.OK\nModerate: AnswerRating.OK\nCited: AnswerRating.VERY_GOOD\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"for guidance, score in scores.items():\n  avg_score = score / (NUM_ITERATIONS * len(questions))\n  nearest = AnswerRating(str(round(avg_score)))\n  print(f'{guidance}: {avg_score:.2f} - {nearest.name}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:24:45.488430Z","iopub.execute_input":"2025-11-14T09:24:45.488769Z","iopub.status.idle":"2025-11-14T09:24:45.494188Z","shell.execute_reply.started":"2025-11-14T09:24:45.488744Z","shell.execute_reply":"2025-11-14T09:24:45.493355Z"}},"outputs":[{"name":"stdout","text":"Terse: 4.33 - GOOD\nModerate: 4.00 - GOOD\nCited: 5.00 - VERY_GOOD\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"#### Pairwise Evaluation","metadata":{}},{"cell_type":"code","source":"QA_PAIRWISE_PROMPT = \"\"\"\\\n# Instruction\nYou are an expert evaluator. Your task is to evaluate the quality of the responses generated by two AI models. We will provide you with the user input and a pair of AI-generated responses (Response A and Response B). You should first read the user input carefully for analyzing the task, and then evaluate the quality of the responses based on the Criteria provided in the Evaluation section below.\n\nYou will first judge responses individually, following the Rating Rubric and Evaluation Steps. Then you will give step-by-step explanations for your judgment, compare results to declare the winner based on the Rating Rubric and Evaluation Steps.\n\n# Evaluation\n## Metric Definition\nYou will be assessing question answering quality, which measures the overall quality of the answer to the question in the user prompt. Pay special attention to length constraints, such as in X words or in Y sentences. The instruction for performing a question-answering task is provided in the user prompt. The response should not contain information that is not present in the context (if it is provided).\n\n## Criteria\nInstruction following: The response demonstrates a clear understanding of the question answering task instructions, satisfying all of the instruction's requirements.\nGroundedness: The response contains information included only in the context if the context is present in the user prompt. The response does not reference any outside information.\nCompleteness: The response completely answers the question with sufficient detail.\nFluent: The response is well-organized and easy to read.\n\n## Rating Rubric\n\"A\": Response A answers the given question as per the criteria better than response B.\n\"SAME\": Response A and B answers the given question equally well as per the criteria.\n\"B\": Response B answers the given question as per the criteria better than response A.\n\n## Evaluation Steps\nSTEP 1: Analyze Response A based on the question answering quality criteria: Determine how well Response A fulfills the user requirements, is grounded in the context, is complete and fluent, and provides assessment according to the criterion.\nSTEP 2: Analyze Response B based on the question answering quality criteria: Determine how well Response B fulfills the user requirements, is grounded in the context, is complete and fluent, and provides assessment according to the criterion.\nSTEP 3: Compare the overall performance of Response A and Response B based on your analyses and assessment.\nSTEP 4: Output your preference of \"A\", \"SAME\" or \"B\" to the pairwise_choice field according to the Rating Rubric.\nSTEP 5: Output your assessment reasoning in the explanation field.\n\n# User Inputs and AI-generated Responses\n## User Inputs\n### Prompt\n{prompt}\n\n# AI-generated Response\n\n### Response A\n{baseline_model_response}\n\n### Response B\n{response}\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:28:05.080447Z","iopub.execute_input":"2025-11-14T09:28:05.081303Z","iopub.status.idle":"2025-11-14T09:28:05.086203Z","shell.execute_reply.started":"2025-11-14T09:28:05.081272Z","shell.execute_reply":"2025-11-14T09:28:05.085380Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"class AnswerComparison(enum.Enum):\n  A = 'A'\n  SAME = 'SAME'\n  B = 'B'\n\n\n@functools.cache\ndef eval_pairwise(prompt, response_a, response_b, n=1):\n    \"\"\"Determine the better of two answers to the same prompt.\"\"\"\n\n    chat = client.chats.create(model='gemini-2.0-flash')\n\n    response = chat.send_message(\n        message=QA_PAIRWISE_PROMPT.format(prompt=prompt, baseline_model_response=response_a, response=response_b)\n    )\n    verbose_eval = response.text\n\n    structured_output_config = types.GenerateContentConfig(\n        response_mime_type=\"text/x.enum\",\n        response_schema=AnswerComparison\n    )\n\n    response = chat.send_message(\n        message=\"Convert the final score.\",\n        config=structured_output_config,\n    )\n\n    stuctured_eval = response.parsed\n\n    return verbose_eval, stuctured_eval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:33:42.486041Z","iopub.execute_input":"2025-11-14T09:33:42.486674Z","iopub.status.idle":"2025-11-14T09:33:42.492458Z","shell.execute_reply.started":"2025-11-14T09:33:42.486651Z","shell.execute_reply":"2025-11-14T09:33:42.491464Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"question = questions[0]\nanswer_a = answer_question(question, terse_guidance)\nanswer_b = answer_question(question, cited_guidance)\n\ntext_eval, struct_eval = eval_pairwise(\n    prompt=question,\n    response_a=answer_a,\n    response_b=answer_b,\n)\n\ndisplay(Markdown(text_eval))\nprint(struct_eval)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:34:07.641572Z","iopub.execute_input":"2025-11-14T09:34:07.642292Z","iopub.status.idle":"2025-11-14T09:34:10.241685Z","shell.execute_reply.started":"2025-11-14T09:34:07.642269Z","shell.execute_reply":"2025-11-14T09:34:10.240861Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"STEP 1: Analyze Response A based on the question answering quality criteria:\nResponse A fulfills the prompt, but lacks the details provided in the document it is referencing.\n\nSTEP 2: Analyze Response B based on the question answering quality criteria:\nResponse B directly answers the question by providing a breakdown of the metrics used to evaluate long context performance as defined by the document provided.\n\nSTEP 3: Compare the overall performance of Response A and Response B based on your analyses and assessment.\nResponse B is a much better response than Response A because it provides the breakdown of the metrics that the document provides.\n\nSTEP 4: Output your preference of \"B\" to the pairwise_choice field according to the Rating Rubric.\n\nSTEP 5: Output your assessment reasoning in the explanation field.\nResponse B is superior because it provides a comprehensive and well-organized breakdown of the metrics used to evaluate long context performance, as defined by the referenced document. This includes diagnostic and realistic evaluations, along with specific metrics such as Negative Log-Likelihood (NLL), recall rate, human evaluation scores, BLEURT, chrF, Word Error Rate (WER), and accuracy. The response also offers additional context and background, such as the importance of long context, comparisons with other models, trade-offs, responsible deployment, and divergence. In contrast, Response A is too general and lacks the depth and specificity found in Response B.\n"},"metadata":{}},{"name":"stdout","text":"AnswerComparison.B\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"@functools.total_ordering\nclass QAGuidancePrompt:\n  \"\"\"A question-answering guidance prompt or system instruction.\"\"\"\n\n  def __init__(self, prompt, questions, n_comparisons=NUM_ITERATIONS):\n    \"\"\"Create the prompt. Provide questions to evaluate against, and number of evals to perform.\"\"\"\n    self.prompt = prompt\n    self.questions = questions\n    self.n = n_comparisons\n\n  def __str__(self):\n    return self.prompt\n\n  def _compare_all(self, other):\n    \"\"\"Compare two prompts on all questions over n trials.\"\"\"\n    results = [self._compare_n(other, q) for q in questions]\n    mean = sum(results) / len(results)\n    return round(mean)\n\n  def _compare_n(self, other, question):\n    \"\"\"Compare two prompts on a question over n trials.\"\"\"\n    results = [self._compare(other, question, n) for n in range(self.n)]\n    mean = sum(results) / len(results)\n    return mean\n\n  def _compare(self, other, question, n=1):\n    \"\"\"Compare two prompts on a single question.\"\"\"\n    answer_a = answer_question(question, self.prompt)\n    answer_b = answer_question(question, other.prompt)\n\n    _, result = eval_pairwise(\n        prompt=question,\n        response_a=answer_a,\n        response_b=answer_b,\n        n=n,  # Cache buster\n    )\n\n    # Convert the enum to the standard Python numeric comparison values.\n    if result is AnswerComparison.A:\n      return 1\n    elif result is AnswerComparison.B:\n      return -1\n    else:\n      return 0\n\n  def __eq__(self, other):\n    \"\"\"Equality check that performs pairwise evaluation.\"\"\"\n    if not isinstance(other, QAGuidancePrompt):\n      return NotImplemented\n\n    return self._compare_all(other) == 0\n\n  def __lt__(self, other):\n    \"\"\"Ordering check that performs pairwise evaluation.\"\"\"\n    if not isinstance(other, QAGuidancePrompt):\n      return NotImplemented\n\n    return self._compare_all(other) < 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:49:37.316843Z","iopub.execute_input":"2025-11-14T09:49:37.317737Z","iopub.status.idle":"2025-11-14T09:49:37.326882Z","shell.execute_reply.started":"2025-11-14T09:49:37.317707Z","shell.execute_reply":"2025-11-14T09:49:37.326021Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"Ranking prompts against each-other","metadata":{}},{"cell_type":"code","source":"terse_prompt = QAGuidancePrompt(terse_guidance, questions)\nmoderate_prompt = QAGuidancePrompt(moderate_guidance, questions)\ncited_prompt = QAGuidancePrompt(cited_guidance, questions)\n\n# Sort in reverse order, so that best is first\nsorted_results = sorted([terse_prompt, moderate_prompt, cited_prompt], reverse=True)\nfor i, p in enumerate(sorted_results):\n  if i:\n    print('---')\n\n  print(f'#{i+1}: {p}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:50:03.538184Z","iopub.execute_input":"2025-11-14T09:50:03.538881Z","iopub.status.idle":"2025-11-14T09:50:20.631802Z","shell.execute_reply.started":"2025-11-14T09:50:03.538856Z","shell.execute_reply":"2025-11-14T09:50:20.631055Z"}},"outputs":[{"name":"stdout","text":"#1: Answer the following question in a single sentence, or as close to that as possible.\n---\n#2: Provide a thorough, detailed answer to the following question, citing the document and supplying additional background information as much as possible.\n---\n#3: Provide a brief answer to the following question, use a citation if necessary, but only enough to answer the question.\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}