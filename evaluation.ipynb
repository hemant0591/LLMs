{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -Uq \"google-genai==1.7.0\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-14T06:05:10.792634Z","iopub.execute_input":"2025-11-14T06:05:10.792929Z","iopub.status.idle":"2025-11-14T06:05:16.971298Z","shell.execute_reply.started":"2025-11-14T06:05:10.792900Z","shell.execute_reply":"2025-11-14T06:05:16.970110Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.18.0 requires google-genai<2.0.0,>=1.45.0, but you have google-genai 1.7.0 which is incompatible.\ngoogle-cloud-aiplatform 1.125.0 requires google-genai<2.0.0,>=1.37.0, but you have google-genai 1.7.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from google import genai\nfrom google.genai import types\n\nfrom IPython.display import Markdown, display\n\ngenai.__version__","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T06:05:54.551153Z","iopub.execute_input":"2025-11-14T06:05:54.552052Z","iopub.status.idle":"2025-11-14T06:05:54.558880Z","shell.execute_reply.started":"2025-11-14T06:05:54.552019Z","shell.execute_reply":"2025-11-14T06:05:54.557872Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'1.7.0'"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nclient = genai.Client(api_key=UserSecretsClient().get_secret(\"GOOGLE_API_KEY\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T06:08:21.309273Z","iopub.execute_input":"2025-11-14T06:08:21.310102Z","iopub.status.idle":"2025-11-14T06:08:21.714810Z","shell.execute_reply.started":"2025-11-14T06:08:21.310067Z","shell.execute_reply":"2025-11-14T06:08:21.713983Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from google.api_core import retry\n\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\nif not hasattr(genai.models.Models.generate_content, '__wrapped__'):\n    genai.models.Models.generate_content = retry.Retry(predicate=is_retriable)(genai.models.Models.generate_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T06:13:48.090348Z","iopub.execute_input":"2025-11-14T06:13:48.090799Z","iopub.status.idle":"2025-11-14T06:13:50.331594Z","shell.execute_reply.started":"2025-11-14T06:13:48.090765Z","shell.execute_reply":"2025-11-14T06:13:50.330710Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### Evaluation\n\nWe'll evaluate a summarisation task using the Gemini 1.5 Pro technical report.","metadata":{}},{"cell_type":"code","source":"!wget -nv -O gemini.pdf https://storage.googleapis.com/cloud-samples-data/generative-ai/pdf/2403.05530.pdf\n\ndocument_file = client.files.upload(file='gemini.pdf')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T06:15:08.032517Z","iopub.execute_input":"2025-11-14T06:15:08.033058Z","iopub.status.idle":"2025-11-14T06:15:09.310754Z","shell.execute_reply.started":"2025-11-14T06:15:08.033033Z","shell.execute_reply":"2025-11-14T06:15:09.309708Z"}},"outputs":[{"name":"stdout","text":"2025-11-14 06:15:08 URL:https://storage.googleapis.com/cloud-samples-data/generative-ai/pdf/2403.05530.pdf [7228817/7228817] -> \"gemini.pdf\" [1]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"#### Summarise a document\nThe summarisation request used here is fairly basic. It targets the training content specifically but provides no guidance otherwise.","metadata":{}},{"cell_type":"code","source":"request = 'Tell me about the training process used here.'\n\ndef summarize_doc(request):\n    \"\"\"Execute the request on the uploaded document.\"\"\"\n    config = types.GenerateContentConfig(temperature=0.0)\n\n    response = client.models.generate_content(\n        model='gemini-2.0-flash',\n        config=config,\n        contents=[request, document_file]\n    )\n\n    return response.text\n\nsummary = summarize_doc(request)\nMarkdown(summary)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T06:18:25.891218Z","iopub.execute_input":"2025-11-14T06:18:25.891641Z","iopub.status.idle":"2025-11-14T06:18:40.112144Z","shell.execute_reply.started":"2025-11-14T06:18:25.891599Z","shell.execute_reply":"2025-11-14T06:18:40.111393Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Based on the document you provided, here's a breakdown of the training process used for Gemini 1.5 Pro:\n\n**1. Data:**\n\n*   **Multimodal and Multilingual Data:** The model is trained on a diverse dataset that includes text, images, audio, and video content. The text data is sourced from various domains, including web documents and code.\n*   **Pre-training Dataset:** The pre-training dataset includes data sourced across many different domains, including web documents and code, and incorporates image, audio, and video content.\n*   **Instruction-Tuning Phase:** Gemini 1.5 Pro is fine-tuned on a collection of multimodal data containing paired instructions and appropriate responses, with further tuning based on human preference data.\n\n**2. Architecture:**\n\n*   **Mixture-of-Experts (MoE) Transformer:** Gemini 1.5 Pro is based on a sparse MoE Transformer architecture. This allows the model to have a large number of parameters while only activating a subset for any given input.\n\n**3. Infrastructure:**\n\n*   **TPUv4 Accelerators:** The model is trained on multiple 4096-chip pods of Google's TPUv4 accelerators, distributed across multiple datacenters.\n\n**4. Training Process:**\n\n*   **Pre-training:** The model is initially pre-trained on the large multimodal dataset.\n*   **Instruction Tuning:** After pre-training, the model is fine-tuned on a collection of multimodal data containing paired instructions and appropriate responses.\n*   **Human Preference Tuning:** Further tuning is performed based on human preference data.\n\n**5. Key Improvements:**\n\n*   **Architecture:** Improvements across the model stack, including architecture, data, optimization, and systems.\n*   **Long-Context Understanding:** Significant architecture changes enable understanding of inputs up to 10 million tokens without performance degradation.\n\n**In summary:** Gemini 1.5 Pro is trained using a large, diverse multimodal dataset on Google's TPUv4 infrastructure. It uses a MoE Transformer architecture and undergoes pre-training, instruction tuning, and human preference tuning. The training process incorporates improvements across the model stack to enable long-context understanding and overall performance."},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"#### Define an evaluator\n\nFor a task like this, we want to evaluate a number of aspects, like how well the model followed the prompt (\"instruction following\"), whether it included relevant data in the prompt (\"groundedness\"), how easy the text is to read (\"fluency\"), or other factors like \"verbosity\" or \"quality\".\n\nIn this step, we define an evaluation agent using a pre-written \"summarisation\" prompt and use it to gauge the quality of the generated summary.","metadata":{}},{"cell_type":"code","source":"SUMMARY_PROMPT = \"\"\"\\\n# Instruction\nYou are an expert evaluator. Your task is to evaluate the quality of the responses generated by AI models.\nWe will provide you with the user input and an AI-generated responses.\nYou should first read the user input carefully for analyzing the task, and then evaluate the quality of the responses based on the Criteria provided in the Evaluation section below.\nYou will assign the response a rating following the Rating Rubric and Evaluation Steps. Give step-by-step explanations for your rating, and only choose ratings from the Rating Rubric.\n\n# Evaluation\n## Metric Definition\nYou will be assessing summarization quality, which measures the overall ability to summarize text. Pay special attention to length constraints, such as in X words or in Y sentences. The instruction for performing a summarization task and the context to be summarized are provided in the user prompt. The response should be shorter than the text in the context. The response should not contain information that is not present in the context.\n\n## Criteria\nInstruction following: The response demonstrates a clear understanding of the summarization task instructions, satisfying all of the instruction's requirements.\nGroundedness: The response contains information included only in the context. The response does not reference any outside information.\nConciseness: The response summarizes the relevant details in the original text without a significant loss in key information without being too verbose or terse.\nFluency: The response is well-organized and easy to read.\n\n## Rating Rubric\n5: (Very good). The summary follows instructions, is grounded, is concise, and fluent.\n4: (Good). The summary follows instructions, is grounded, concise, and fluent.\n3: (Ok). The summary mostly follows instructions, is grounded, but is not very concise and is not fluent.\n2: (Bad). The summary is grounded, but does not follow the instructions.\n1: (Very bad). The summary is not grounded.\n\n## Evaluation Steps\nSTEP 1: Assess the response in aspects of instruction following, groundedness, conciseness, and verbosity according to the criteria.\nSTEP 2: Score based on the rubric.\n\n# User Inputs and AI-generated Response\n## User Inputs\n\n### Prompt\n{prompt}\n\n## AI-generated Response\n{response}\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T06:26:46.430172Z","iopub.execute_input":"2025-11-14T06:26:46.430919Z","iopub.status.idle":"2025-11-14T06:26:46.436323Z","shell.execute_reply.started":"2025-11-14T06:26:46.430890Z","shell.execute_reply":"2025-11-14T06:26:46.435140Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import enum\n\n# Define a structured enum class to capture the result.\nclass SummaryRating(enum.Enum):\n  VERY_GOOD = '5'\n  GOOD = '4'\n  OK = '3'\n  BAD = '2'\n  VERY_BAD = '1'\n\ndef eval_summary(prompt, ai_response):\n    \"\"\"Evaluate the generated summary against the prompt used.\"\"\"\n\n    chat = client.chats.create(model='gemini-2.0-flash')\n\n    #generate full chat response\n    response = chat.send_message(\n        message=SUMMARY_PROMPT.format(prompt=prompt, response=ai_response)\n    )\n\n    verbose_eval = response.text\n\n    # get desired structure\n    structured_output_config = types.GenerateContentConfig(\n        response_mime_type=\"text/x.enum\",\n        response_schema=SummaryRating,\n        )\n\n    response = chat.send_message(\n      message=\"Convert the final score.\",\n      config=structured_output_config,\n      )\n    structured_eval = response.parsed\n\n    return verbose_eval, structured_eval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T06:34:37.760083Z","iopub.execute_input":"2025-11-14T06:34:37.760852Z","iopub.status.idle":"2025-11-14T06:34:37.767435Z","shell.execute_reply.started":"2025-11-14T06:34:37.760816Z","shell.execute_reply":"2025-11-14T06:34:37.766486Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"text_eval, struct_eval = eval_summary(prompt=[request, document_file], ai_response=summary)\nMarkdown(text_eval)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T06:38:53.546296Z","iopub.execute_input":"2025-11-14T06:38:53.546668Z","iopub.status.idle":"2025-11-14T06:38:54.945161Z","shell.execute_reply.started":"2025-11-14T06:38:53.546638Z","shell.execute_reply":"2025-11-14T06:38:54.944319Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"## Evaluation\nSTEP 1:\nThe response summarizes the training process well. It follows instructions and provides a comprehensive overview of the data, architecture, infrastructure, training process, and key improvements. The response is grounded and provides accurate details from the document. The information is presented in a well-organized and easy-to-read manner.\n\nSTEP 2:\nRating: 5\n"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"struct_eval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T06:40:39.377464Z","iopub.execute_input":"2025-11-14T06:40:39.377831Z","iopub.status.idle":"2025-11-14T06:40:39.384461Z","shell.execute_reply.started":"2025-11-14T06:40:39.377799Z","shell.execute_reply":"2025-11-14T06:40:39.383583Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"<SummaryRating.VERY_GOOD: '5'>"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"new_prompt = \"Explain like I'm 5 the training process\"\n\nif not new_prompt:\n  raise ValueError(\"Try setting a new summarisation prompt.\")\n\ndef run_and_eval_summary(prompt):\n  \"\"\"Generate and evaluate the summary using the new prompt.\"\"\"\n  summary = summarize_doc(new_prompt)\n  display(Markdown(summary + '\\n-----'))\n\n  text, struct = eval_summary([new_prompt, document_file], summary)\n  display(Markdown(text + '\\n-----'))\n  print(struct)\n\nrun_and_eval_summary(new_prompt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T06:43:13.254941Z","iopub.execute_input":"2025-11-14T06:43:13.255224Z","iopub.status.idle":"2025-11-14T06:43:28.257284Z","shell.execute_reply.started":"2025-11-14T06:43:13.255205Z","shell.execute_reply":"2025-11-14T06:43:28.256370Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Okay, I can explain the training process of a large language model like Gemini 1.5 in a way that a 5-year-old can understand.\n\nImagine you have a puppy, and you want to teach it to understand and follow your instructions.\n\n1.  **Gathering Lots of Examples (Data):** First, you need to show the puppy lots and lots of examples. You might show it pictures of cats and say \"cat,\" pictures of dogs and say \"dog,\" and so on. For Gemini, this means feeding it tons of text, pictures, videos, and sounds from the internet and books. It's like showing the puppy everything in the world!\n\n2.  **Teaching the Puppy (Training):** Now, you start teaching the puppy what things mean. You might say, \"Fetch the ball!\" and then reward the puppy with a treat when it brings you the ball. For Gemini, this means the computer is learning to predict what word or picture comes next in a sequence. If it predicts correctly, it gets a \"treat\" (a small adjustment to its internal settings).\n\n3.  **Making the Puppy Smarter (Fine-tuning):** After the puppy knows the basics, you can teach it more complicated tricks. You might say, \"Sit and stay!\" and then reward the puppy when it does both things correctly. For Gemini, this means giving it specific instructions and examples of how to answer questions, write stories, or translate languages.\n\n4.  **Testing the Puppy (Evaluation):** Finally, you need to test the puppy to see if it has learned everything correctly. You might give it a new command and see if it follows it. For Gemini, this means giving it tests to see if it can answer questions correctly, write good stories, and translate languages accurately.\n\nSo, the training process is all about showing the computer lots of examples, teaching it what things mean, making it smarter with specific instructions, and then testing it to see if it has learned everything correctly. It's like training a puppy, but with computers instead of dogs!\n-----"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"## Evaluation\n\n### STEP 1: Assess the response in aspects of instruction following, groundedness, conciseness, and verbosity according to the criteria.\nInstruction following: The response follows the instruction of explaining like I'm 5.\nGroundedness: The response contains information included only in the context. The response does not reference any outside information.\nConciseness: The response is concise.\nFluency: The response is well-organized and easy to read.\n\n### STEP 2: Score based on the rubric.\n5\n\n-----"},"metadata":{}},{"name":"stdout","text":"SummaryRating.VERY_GOOD\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}